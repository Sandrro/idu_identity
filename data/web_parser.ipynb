{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b509935",
   "metadata": {},
   "source": [
    "# Сбор текстов с веб-страниц (парсер) — интерактивный режим\n",
    "\n",
    "Этот блокнот предназначен для студентов без знания Python: на “фронте” есть кнопки, которые **выполняют сбор текстов** с веб-страниц и формируют таблицу `docs`.\n",
    "\n",
    "## Что делает\n",
    "- скачивает HTML по URL (requests)\n",
    "- извлекает **основной текст** (эвристика: `article/main` → иначе “крупнейший текстовый блок”)\n",
    "- достаёт `title`, `date` (если есть)\n",
    "- собирает единый датасет `docs`\n",
    "\n",
    "## Выход `docs`\n",
    "Колонки: `doc_id`, `source`, `text_raw`, `text_clean`, `date`, `url`, `meta`\n",
    "\n",
    "> Примечание: некоторые сайты могут блокировать запросы. Тогда в `meta.fetch.error` будет причина.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51829d48",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import hashlib\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722fc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "_is_colab = importlib.util.find_spec('google.colab') is not None\n",
    "if _is_colab:\n",
    "    from google.colab import output  # type: ignore\n",
    "    output.enable_custom_widget_manager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dea316",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Мини-очистка текста\n",
    "# ----------------------------\n",
    "_HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "_SPACE_RE = re.compile(r\"\\s+\")\n",
    "_PUNCT_RUN_RE = re.compile(r\"([!?.,])\\1{2,}\")\n",
    "\n",
    "def clean_text_minimal(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    t = str(text)\n",
    "    t = _HTML_TAG_RE.sub(\" \", t)\n",
    "    t = _PUNCT_RUN_RE.sub(r\"\\1\\1\", t)\n",
    "    t = _SPACE_RE.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def _stable_id(url: str, text: str) -> str:\n",
    "    h = hashlib.sha1((url + \"\\n\" + (text or \"\")[:4000]).encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:16]\n",
    "    return f\"web_{h}\"\n",
    "\n",
    "def _safe_to_datetime(value: Optional[str]) -> Optional[pd.Timestamp]:\n",
    "    if not value:\n",
    "        return None\n",
    "    dt = pd.to_datetime(value, errors=\"coerce\", utc=True)\n",
    "    if pd.isna(dt):\n",
    "        return None\n",
    "    return dt\n",
    "\n",
    "# ----------------------------\n",
    "# Извлечение содержимого из HTML\n",
    "# ----------------------------\n",
    "def _strip_noise(soup: BeautifulSoup) -> None:\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\", \"canvas\", \"iframe\"]):\n",
    "        tag.decompose()\n",
    "    for tag in soup.find_all([\"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "        tag.decompose()\n",
    "    for c in soup.find_all(string=lambda x: isinstance(x, Comment)):\n",
    "        c.extract()\n",
    "\n",
    "def _get_title(soup: BeautifulSoup) -> Optional[str]:\n",
    "    for attr, val in [(\"property\", \"og:title\"), (\"name\", \"twitter:title\")]:\n",
    "        node = soup.find(\"meta\", attrs={attr: val})\n",
    "        if node and node.get(\"content\"):\n",
    "            return clean_text_minimal(node[\"content\"])\n",
    "\n",
    "    if soup.title and soup.title.get_text(strip=True):\n",
    "        return clean_text_minimal(soup.title.get_text(\" \", strip=True))\n",
    "\n",
    "    h1 = soup.find(\"h1\")\n",
    "    if h1 and h1.get_text(strip=True):\n",
    "        return clean_text_minimal(h1.get_text(\" \", strip=True))\n",
    "\n",
    "    return None\n",
    "\n",
    "def _get_date(soup: BeautifulSoup) -> Optional[pd.Timestamp]:\n",
    "    meta_candidates = [\n",
    "        (\"property\", \"article:published_time\"),\n",
    "        (\"property\", \"article:modified_time\"),\n",
    "        (\"property\", \"og:updated_time\"),\n",
    "        (\"name\", \"pubdate\"),\n",
    "        (\"name\", \"publishdate\"),\n",
    "        (\"name\", \"timestamp\"),\n",
    "        (\"name\", \"date\"),\n",
    "        (\"name\", \"DC.date.issued\"),\n",
    "        (\"name\", \"DC.Date\"),\n",
    "        (\"itemprop\", \"datePublished\"),\n",
    "        (\"itemprop\", \"dateModified\"),\n",
    "    ]\n",
    "    for attr, val in meta_candidates:\n",
    "        node = soup.find(\"meta\", attrs={attr: val})\n",
    "        if node and node.get(\"content\"):\n",
    "            dt = _safe_to_datetime(node[\"content\"])\n",
    "            if dt is not None:\n",
    "                return dt\n",
    "\n",
    "    time_tag = soup.find(\"time\")\n",
    "    if time_tag:\n",
    "        dt = _safe_to_datetime(time_tag.get(\"datetime\"))\n",
    "        if dt is not None:\n",
    "            return dt\n",
    "    return None\n",
    "\n",
    "def _node_text_len(node) -> int:\n",
    "    txt = node.get_text(\" \", strip=True) if node else \"\"\n",
    "    txt = clean_text_minimal(txt)\n",
    "    return len(txt)\n",
    "\n",
    "def _extract_main_text(\n",
    "    soup: BeautifulSoup,\n",
    "    selector: Optional[str] = None,\n",
    "    min_chars: int = 400,\n",
    ") -> Tuple[str, Dict[str, Any]]:\n",
    "    meta: Dict[str, Any] = {\"extractor\": None}\n",
    "\n",
    "    if selector:\n",
    "        nodes = soup.select(selector)\n",
    "        if nodes:\n",
    "            parts = [n.get_text(\"\\n\", strip=True) for n in nodes]\n",
    "            text = clean_text_minimal(\"\\n\".join(parts))\n",
    "            meta[\"extractor\"] = f\"css:{selector}\"\n",
    "            if len(text) >= min_chars:\n",
    "                return text, meta\n",
    "            meta[\"extractor_fallback\"] = \"too_short\"\n",
    "\n",
    "    for tag_name in [\"article\", \"main\"]:\n",
    "        node = soup.find(tag_name)\n",
    "        if node:\n",
    "            text = clean_text_minimal(node.get_text(\"\\n\", strip=True))\n",
    "            if len(text) >= min_chars:\n",
    "                meta[\"extractor\"] = tag_name\n",
    "                return text, meta\n",
    "\n",
    "    candidates = []\n",
    "    for key in [\"content\", \"article\", \"post\", \"entry\", \"text\", \"body\", \"main\"]:\n",
    "        candidates.extend(soup.find_all(attrs={\"class\": re.compile(key, re.I)}))\n",
    "        candidates.extend(soup.find_all(attrs={\"id\": re.compile(key, re.I)}))\n",
    "\n",
    "    candidates.extend(soup.find_all([\"div\", \"section\"]))\n",
    "\n",
    "    best = None\n",
    "    best_len = 0\n",
    "    for node in candidates:\n",
    "        l = _node_text_len(node)\n",
    "        if l > best_len:\n",
    "            best = node\n",
    "            best_len = l\n",
    "\n",
    "    if best is not None and best_len > 0:\n",
    "        meta[\"extractor\"] = \"largest_block\"\n",
    "        text = clean_text_minimal(best.get_text(\"\\n\", strip=True))\n",
    "        return text, meta\n",
    "\n",
    "    meta[\"extractor\"] = \"none\"\n",
    "    return \"\", meta\n",
    "\n",
    "# ----------------------------\n",
    "# HTTP\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class FetchConfig:\n",
    "    timeout: int = 20\n",
    "    user_agent: str = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari\"\n",
    "    max_bytes: int = 5_000_000  # 5MB\n",
    "\n",
    "def _fetch_html(url: str, cfg: FetchConfig, session: requests.Session) -> Tuple[Optional[str], Dict[str, Any]]:\n",
    "    meta: Dict[str, Any] = {\"status_code\": None, \"final_url\": None}\n",
    "    try:\n",
    "        r = session.get(\n",
    "            url,\n",
    "            headers={\"User-Agent\": cfg.user_agent, \"Accept\": \"text/html,application/xhtml+xml\"},\n",
    "            timeout=cfg.timeout,\n",
    "            allow_redirects=True,\n",
    "        )\n",
    "        meta[\"status_code\"] = r.status_code\n",
    "        meta[\"final_url\"] = r.url\n",
    "        r.raise_for_status()\n",
    "\n",
    "        content = r.content\n",
    "        if content and len(content) > cfg.max_bytes:\n",
    "            meta[\"error\"] = f\"response_too_large:{len(content)}\"\n",
    "            return None, meta\n",
    "\n",
    "        return r.text, meta\n",
    "    except Exception as e:\n",
    "        meta[\"error\"] = repr(e)\n",
    "        return None, meta\n",
    "\n",
    "def parse_websites(\n",
    "    urls: List[str],\n",
    "    *,\n",
    "    selector: Optional[str] = None,\n",
    "    min_chars: int = 400,\n",
    "    cfg: Optional[FetchConfig] = None,\n",
    ") -> pd.DataFrame:\n",
    "    cfg = cfg or FetchConfig()\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        for url in urls:\n",
    "            url = (url or \"\").strip()\n",
    "            if not url:\n",
    "                continue\n",
    "\n",
    "            html, fetch_meta = _fetch_html(url, cfg, session=session)\n",
    "            if not html:\n",
    "                rows.append({\n",
    "                    \"doc_id\": _stable_id(url, \"\"),\n",
    "                    \"source\": \"website\",\n",
    "                    \"text_raw\": \"\",\n",
    "                    \"date\": None,\n",
    "                    \"url\": url,\n",
    "                    \"meta\": {\"fetch\": fetch_meta},\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "            _strip_noise(soup)\n",
    "\n",
    "            title = _get_title(soup)\n",
    "            date = _get_date(soup)\n",
    "            text_raw, extract_meta = _extract_main_text(soup, selector=selector, min_chars=min_chars)\n",
    "\n",
    "            meta = {\n",
    "                \"fetch\": fetch_meta,\n",
    "                \"title\": title,\n",
    "                \"date_extracted\": date.isoformat() if isinstance(date, pd.Timestamp) else None,\n",
    "                \"extraction\": extract_meta,\n",
    "                \"domain\": urlparse(fetch_meta.get(\"final_url\") or url).netloc,\n",
    "            }\n",
    "\n",
    "            rows.append({\n",
    "                \"doc_id\": _stable_id(fetch_meta.get(\"final_url\") or url, text_raw),\n",
    "                \"source\": \"website\",\n",
    "                \"text_raw\": text_raw,\n",
    "                \"date\": date,\n",
    "                \"url\": fetch_meta.get(\"final_url\") or url,\n",
    "                \"meta\": meta,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame(columns=[\"doc_id\", \"source\", \"text_raw\", \"date\", \"url\", \"meta\"])\n",
    "\n",
    "    df[\"text_clean\"] = df[\"text_raw\"].map(clean_text_minimal)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87cc382",
   "metadata": {},
   "source": [
    "## Интерфейс (кнопки)\n",
    "\n",
    "1) Вставь URL-ы по одному на строку  \n",
    "2) (Опционально) укажи CSS-селектор (например `article`, `.post-content`, `main`)  \n",
    "3) Нажми **Собрать тексты**  \n",
    "4) По желанию: **Скачать CSV**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f7cb5",
   "metadata": {
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "#@title Форма управления { display-mode: \"form\" }\n",
    "# --- UI controls ---\n",
    "urls_ta = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder=\"https://example.com/article1\\nhttps://example.com/article2\",\n",
    "    description=\"URL-ы:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"900px\", height=\"160px\"),\n",
    ")\n",
    "\n",
    "selector_txt = widgets.Text(\n",
    "    value=\"\",\n",
    "    placeholder=\"(опционально) например: article или .post-content\",\n",
    "    description=\"CSS селектор:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"560px\"),\n",
    ")\n",
    "\n",
    "min_chars_int = widgets.IntSlider(\n",
    "    value=400, min=100, max=4000, step=50,\n",
    "    description=\"Мин. длина текста:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"560px\"),\n",
    ")\n",
    "\n",
    "timeout_int = widgets.IntSlider(\n",
    "    value=20, min=5, max=120, step=5,\n",
    "    description=\"Таймаут (сек):\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"560px\"),\n",
    ")\n",
    "\n",
    "parse_btn = widgets.Button(description=\"Собрать тексты\", button_style=\"primary\", icon=\"play\")\n",
    "demo_btn = widgets.Button(description=\"Подставить DEMO URL\", button_style=\"info\")\n",
    "save_btn = widgets.Button(description=\"Скачать CSV\", button_style=\"success\", icon=\"download\")\n",
    "preview_btn = widgets.Button(description=\"Показать 1-й текст\", button_style=\"\")\n",
    "\n",
    "status_out = widgets.Output()\n",
    "table_out = widgets.Output()\n",
    "preview_out = widgets.Output()\n",
    "\n",
    "docs = None  # will hold DataFrame\n",
    "\n",
    "def _get_urls() -> List[str]:\n",
    "    return [u.strip() for u in urls_ta.value.splitlines() if u.strip()]\n",
    "\n",
    "def _set_demo(_=None):\n",
    "    urls_ta.value = \"\"\"https://example.com\n",
    "https://www.iana.org/domains/reserved\"\"\"\n",
    "\n",
    "def _parse(_=None):\n",
    "    global docs\n",
    "    urls = _get_urls()\n",
    "    cfg = FetchConfig(timeout=int(timeout_int.value))\n",
    "    selector = selector_txt.value.strip() or None\n",
    "    min_chars = int(min_chars_int.value)\n",
    "\n",
    "    with status_out:\n",
    "        status_out.clear_output()\n",
    "        display(Markdown(\n",
    "            f\"**URL-ов:** {len(urls)}  \\\\n\"\n",
    "\n",
    "            f\"**selector:** `{selector}`  \\\\n\"\n",
    "\n",
    "            f\"**min_chars:** {min_chars}  \\\\n\"\n",
    "\n",
    "            f\"**timeout:** {cfg.timeout}s\"\n",
    "\n",
    "        ))\n",
    "\n",
    "    with table_out:\n",
    "        table_out.clear_output()\n",
    "        if not urls:\n",
    "            display(Markdown(\"⚠️ Список URL пуст.\"))\n",
    "            return\n",
    "        docs = parse_websites(urls, selector=selector, min_chars=min_chars, cfg=cfg)\n",
    "        display(Markdown(f\"✅ Готово. Документов: **{len(docs)}**\"))\n",
    "        display(docs[[\"doc_id\", \"source\", \"date\", \"url\"]].head(50))\n",
    "\n",
    "def _preview(_=None):\n",
    "    global docs\n",
    "    with preview_out:\n",
    "        preview_out.clear_output()\n",
    "        if docs is None or docs.empty:\n",
    "            display(Markdown(\"⚠️ Сначала нажми **Собрать тексты**.\"))\n",
    "            return\n",
    "        row = docs.iloc[0].to_dict()\n",
    "        title = (row.get('meta') or {}).get('title') or 'Без заголовка'\n",
    "        display(Markdown(f\"### {title}\"))\n",
    "        display(Markdown(f\"**URL:** {row.get('url')}\"))\n",
    "        txt = (row.get(\"text_raw\") or \"\").strip()\n",
    "        if not txt:\n",
    "            display(Markdown(\"(текст не извлечён — см. `meta.fetch.error`)\"))\n",
    "        else:\n",
    "            display(Markdown(txt[:2500] + (\"…\" if len(txt) > 2500 else \"\")))\n",
    "\n",
    "def _save(_=None):\n",
    "    global docs\n",
    "    with status_out:\n",
    "        if docs is None or docs.empty:\n",
    "            display(Markdown(\"⚠️ Нечего сохранять. Сначала нажми **Собрать тексты**.\"))\n",
    "            return\n",
    "        path = \"docs_web.csv\"\n",
    "        docs.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "        display(Markdown(f\"✅ Сохранено в файл: `{path}` (скачай через файловый браузер Jupyter/Colab).\"))\n",
    "\n",
    "demo_btn.on_click(_set_demo)\n",
    "parse_btn.on_click(_parse)\n",
    "preview_btn.on_click(_preview)\n",
    "save_btn.on_click(_save)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    urls_ta,\n",
    "    widgets.HBox([demo_btn, parse_btn, preview_btn, save_btn]),\n",
    "    widgets.HBox([selector_txt, min_chars_int]),\n",
    "    timeout_int,\n",
    "    status_out,\n",
    "    table_out,\n",
    "    preview_out,\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0d549",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Типичные проблемы\n",
    "- Если `text_raw` пустой: сайт мог отдать капчу/редирект/заблокировать запрос — смотри `meta.fetch.error`.\n",
    "- Если вытаскивает мусор: попробуй заполнить **CSS селектор** (`article`, `.post-content`, `main` и т.п.).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
