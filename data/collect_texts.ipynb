{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba04c4a3",
   "metadata": {},
   "source": [
    "\n",
    "# Раздел 1 — Сбор текстов (VK / сайты / Яндекс Отзывы)\n",
    "\n",
    "В этом разделе мы приводим тексты из разных источников к **единому формату** таблицы `docs`.\n",
    "\n",
    "- Блокнот объединяет общий сбор (VK, сайты, Яндекс) и отдельный **парсер веб-страниц** из второго блокнота.\n",
    "- Можно вводить ссылки вручную или загрузить **XLSX со списком ссылок**.\n",
    "- Для VK при наличии токена используется **SOIKA** (установка через pip).\n",
    "\n",
    "## Ограничения курса\n",
    "- Используем **только тексты**.\n",
    "- **LLM запрещены**.\n",
    "- Никаких внешних баз, требующих предварительной разметки.\n",
    "- Допустимо: классические библиотеки + BERT/эмбеддинги и методы на их основе (в следующих разделах).\n",
    "\n",
    "## Единая схема данных `docs`\n",
    "- `doc_id`: уникальный ID документа\n",
    "- `source`: `vk | website | yandex_reviews`\n",
    "- `text_raw`: исходный текст\n",
    "- `text_clean`: очищенный текст (в этом разделе — простая базовая очистка)\n",
    "- `date`: дата (если есть)\n",
    "- `url`: ссылка на источник (если есть)\n",
    "- `meta`: словарь с доп. полями (например, группа VK, рейтинг, заголовок)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e9b86c",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import io\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962cd86",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# Попытка импортировать SOIKA (используется для VK). Если недоступна — будет заглушка.\n",
    "try:  # pragma: no cover - интерактивный импорт\n",
    "    from soika import VKParser  # type: ignore\n",
    "    _soika_import_error = None\n",
    "except Exception as e:  # pragma: no cover - интерактивный импорт\n",
    "    VKParser = None  # type: ignore\n",
    "    _soika_import_error = e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d641685",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "_is_colab = importlib.util.find_spec('google.colab') is not None\n",
    "if _is_colab:\n",
    "    from google.colab import output  # type: ignore\n",
    "    output.enable_custom_widget_manager()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6ae83e",
   "metadata": {},
   "source": [
    "\n",
    "## Панель управления\n",
    "Выберите источник и введите параметры. Можно вставить ссылки вручную **или** загрузить XLSX со ссылками. Для демонстрации можно нажать **«Загрузить DEMO-корпус»**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2216832",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "#@title Форма управления { display-mode: \"form\" }\n",
    "source_dd = widgets.Dropdown(\n",
    "    options=[(\"VK (стены групп)\", \"vk\"), (\"Сайты\", \"website\"), (\"Яндекс Отзывы\", \"yandex_reviews\")],\n",
    "    value=\"vk\",\n",
    "    description=\"Источник:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"420px\"),\n",
    ")\n",
    "\n",
    "input_ta = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder=\"Для VK: ссылки на группы или домены через запятую/строку\n",
    "Для сайтов: URL-ы по строкам\n",
    "Для Яндекс Отзывов: URL-ы по строкам\",\n",
    "    description=\"Ввод вручную:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"820px\", height=\"140px\"),\n",
    ")\n",
    "\n",
    "single_link_txt = widgets.Text(\n",
    "    value=\"\",\n",
    "    placeholder=\"Быстрая вставка одной ссылки\",\n",
    "    description=\"Одна ссылка:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"560px\"),\n",
    ")\n",
    "\n",
    "links_upload = widgets.FileUpload(\n",
    "    accept=\".xlsx\",\n",
    "    multiple=False,\n",
    "    description=\"XLSX со ссылками\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "since_txt = widgets.Text(\n",
    "    value=\"\",\n",
    "    placeholder=\"YYYY-MM-DD (необязательно)\",\n",
    "    description=\"Период с:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"260px\"),\n",
    ")\n",
    "\n",
    "until_txt = widgets.Text(\n",
    "    value=\"\",\n",
    "    placeholder=\"YYYY-MM-DD (необязательно)\",\n",
    "    description=\"Период по:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"260px\"),\n",
    ")\n",
    "\n",
    "vk_token_txt = widgets.Password(\n",
    "    value=\"\",\n",
    "    placeholder=\"Токен VK для SOIKA (https://dev.vk.com/api/access-token)\",\n",
    "    description=\"VK токен:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"480px\"),\n",
    ")\n",
    "\n",
    "selector_main = widgets.Text(\n",
    "    value=\"\",\n",
    "    placeholder=\"Для сайтов: article / .post-content / main\",\n",
    "    description=\"CSS селектор:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"420px\"),\n",
    ")\n",
    "\n",
    "min_chars_main = widgets.IntSlider(\n",
    "    value=400, min=100, max=4000, step=50,\n",
    "    description=\"Мин. длина текста:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"420px\"),\n",
    ")\n",
    "\n",
    "timeout_main = widgets.IntSlider(\n",
    "    value=20, min=5, max=120, step=5,\n",
    "    description=\"Таймаут (сек):\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"420px\"),\n",
    ")\n",
    "\n",
    "demo_btn = widgets.Button(description=\"Загрузить DEMO-корпус\", button_style=\"info\")\n",
    "run_btn = widgets.Button(description=\"Запустить сбор\", button_style=\"primary\")\n",
    "\n",
    "out = widgets.Output()\n",
    "\n",
    "website_opts = widgets.Accordion(children=[widgets.VBox([selector_main, min_chars_main, timeout_main])])\n",
    "website_opts.set_title(0, \"Опции веб-парсера (для источника 'Сайты')\")\n",
    "\n",
    "vk_opts = widgets.Accordion(children=[vk_token_txt])\n",
    "vk_opts.set_title(0, \"Параметры VK / SOIKA\")\n",
    "\n",
    "upload_box = widgets.VBox([\n",
    "    widgets.HBox([links_upload, single_link_txt]),\n",
    "    input_ta,\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e23efb",
   "metadata": {},
   "source": [
    "## Базовая очистка текста\n",
    "Пока что делаем минимально:\n",
    "- убираем HTML-теги,\n",
    "- нормализуем пробелы,\n",
    "- схлопываем повторяющиеся знаки.\n",
    "\n",
    "Более серьёзная нормализация (леммы, дедупликация и т.д.) — в следующем разделе «Подготовка корпуса»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278c18e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "SPACE_RE = re.compile(r\"\\s+\")\n",
    "PUNCT_RUN_RE = re.compile(r\"([!?.,])\\1{2,}\")\n",
    "\n",
    "def clean_text_minimal(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    t = str(text)\n",
    "    t = HTML_TAG_RE.sub(\" \", t)\n",
    "    t = PUNCT_RUN_RE.sub(r\"\\1\\1\", t)\n",
    "    t = SPACE_RE.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def parse_date_safe(s: str) -> Optional[pd.Timestamp]:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return pd.to_datetime(s)\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a56bd",
   "metadata": {},
   "source": [
    "\n",
    "## Инструмент: парсер веб-страниц (встроен)\n",
    "Функции ниже — из объединённого веб-парсера. Они поддерживают выбор CSS-селектора, таймаут и фильтр по длине текста. Используются как в общей форме, так и в отдельном UI ниже.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a258c2e",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Извлечение содержимого из HTML\n",
    "# ----------------------------\n",
    "\n",
    "def _strip_noise(soup: BeautifulSoup) -> None:\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\", \"canvas\", \"iframe\"]):\n",
    "        tag.decompose()\n",
    "    for tag in soup.find_all([\"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "        tag.decompose()\n",
    "    for c in soup.find_all(string=lambda x: isinstance(x, Comment)):\n",
    "        c.extract()\n",
    "\n",
    "def _get_title(soup: BeautifulSoup) -> Optional[str]:\n",
    "    for attr, val in [(\"property\", \"og:title\"), (\"name\", \"twitter:title\")]:\n",
    "        node = soup.find(\"meta\", attrs={attr: val})\n",
    "        if node and node.get(\"content\"):\n",
    "            return clean_text_minimal(node[\"content\"])\n",
    "\n",
    "    if soup.title and soup.title.get_text(strip=True):\n",
    "        return clean_text_minimal(soup.title.get_text(\" \", strip=True))\n",
    "\n",
    "    h1 = soup.find(\"h1\")\n",
    "    if h1 and h1.get_text(strip=True):\n",
    "        return clean_text_minimal(h1.get_text(\" \", strip=True))\n",
    "\n",
    "    return None\n",
    "\n",
    "def _safe_to_datetime(value: Optional[str]) -> Optional[pd.Timestamp]:\n",
    "    if not value:\n",
    "        return None\n",
    "    dt = pd.to_datetime(value, errors=\"coerce\", utc=True)\n",
    "    if pd.isna(dt):\n",
    "        return None\n",
    "    return dt\n",
    "\n",
    "def _get_date(soup: BeautifulSoup) -> Optional[pd.Timestamp]:\n",
    "    meta_candidates = [\n",
    "        (\"property\", \"article:published_time\"),\n",
    "        (\"property\", \"article:modified_time\"),\n",
    "        (\"property\", \"og:updated_time\"),\n",
    "        (\"name\", \"pubdate\"),\n",
    "        (\"name\", \"publishdate\"),\n",
    "        (\"name\", \"timestamp\"),\n",
    "        (\"name\", \"date\"),\n",
    "        (\"name\", \"DC.date.issued\"),\n",
    "        (\"name\", \"DC.Date\"),\n",
    "        (\"itemprop\", \"datePublished\"),\n",
    "        (\"itemprop\", \"dateModified\"),\n",
    "    ]\n",
    "    for attr, val in meta_candidates:\n",
    "        node = soup.find(\"meta\", attrs={attr: val})\n",
    "        if node and node.get(\"content\"):\n",
    "            dt = _safe_to_datetime(node[\"content\"])\n",
    "            if dt is not None:\n",
    "                return dt\n",
    "\n",
    "    time_tag = soup.find(\"time\")\n",
    "    if time_tag:\n",
    "        dt = _safe_to_datetime(time_tag.get(\"datetime\"))\n",
    "        if dt is not None:\n",
    "            return dt\n",
    "    return None\n",
    "\n",
    "def _node_text_len(node) -> int:\n",
    "    if not hasattr(node, \"get_text\"):\n",
    "        return 0\n",
    "    return len(node.get_text(\" \", strip=True))\n",
    "\n",
    "def _extract_main_text(\n",
    "    soup: BeautifulSoup, *, selector: Optional[str] = None, min_chars: int = 400\n",
    ") -> Tuple[str, Dict[str, Any]]:\n",
    "    meta: Dict[str, Any] = {\"extractor\": None}\n",
    "\n",
    "    if selector:\n",
    "        nodes = soup.select(selector)\n",
    "        if nodes:\n",
    "            parts = [n.get_text(\"\n",
    "\", strip=True) for n in nodes]\n",
    "            text = clean_text_minimal(\"\n",
    "\".join(parts))\n",
    "            meta[\"extractor\"] = f\"css:{selector}\"\n",
    "            if len(text) >= min_chars:\n",
    "                return text, meta\n",
    "            meta[\"extractor_fallback\"] = \"too_short\"\n",
    "\n",
    "    for tag_name in [\"article\", \"main\"]:\n",
    "        node = soup.find(tag_name)\n",
    "        if node:\n",
    "            text = clean_text_minimal(node.get_text(\"\n",
    "\", strip=True))\n",
    "            if len(text) >= min_chars:\n",
    "                meta[\"extractor\"] = tag_name\n",
    "                return text, meta\n",
    "\n",
    "    candidates = []\n",
    "    for key in [\"content\", \"article\", \"post\", \"entry\", \"text\", \"body\", \"main\"]:\n",
    "        candidates.extend(soup.find_all(attrs={\"class\": re.compile(key, re.I)}))\n",
    "        candidates.extend(soup.find_all(attrs={\"id\": re.compile(key, re.I)}))\n",
    "\n",
    "    candidates.extend(soup.find_all([\"div\", \"section\"]))\n",
    "\n",
    "    best = None\n",
    "    best_len = 0\n",
    "    for node in candidates:\n",
    "        l = _node_text_len(node)\n",
    "        if l > best_len:\n",
    "            best = node\n",
    "            best_len = l\n",
    "\n",
    "    if best is not None and best_len > 0:\n",
    "        meta[\"extractor\"] = \"largest_block\"\n",
    "        text = clean_text_minimal(best.get_text(\"\n",
    "\", strip=True))\n",
    "        return text, meta\n",
    "\n",
    "    meta[\"extractor\"] = \"none\"\n",
    "    return \"\", meta\n",
    "\n",
    "# ----------------------------\n",
    "# HTTP helpers\n",
    "# ----------------------------\n",
    "\n",
    "@dataclass\n",
    "class FetchConfig:\n",
    "    timeout: int = 20\n",
    "    user_agent: str = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari\"\n",
    "    max_bytes: int = 5_000_000  # 5MB\n",
    "\n",
    "def _fetch_html(url: str, cfg: FetchConfig, session: requests.Session) -> Tuple[Optional[str], Dict[str, Any]]:\n",
    "    meta: Dict[str, Any] = {\"status_code\": None, \"final_url\": None}\n",
    "    try:\n",
    "        r = session.get(\n",
    "            url,\n",
    "            headers={\"User-Agent\": cfg.user_agent, \"Accept\": \"text/html,application/xhtml+xml\"},\n",
    "            timeout=cfg.timeout,\n",
    "            allow_redirects=True,\n",
    "        )\n",
    "        meta[\"status_code\"] = r.status_code\n",
    "        meta[\"final_url\"] = r.url\n",
    "        r.raise_for_status()\n",
    "\n",
    "        content = r.content\n",
    "        if content and len(content) > cfg.max_bytes:\n",
    "            meta[\"error\"] = f\"response_too_large:{len(content)}\"\n",
    "            return None, meta\n",
    "\n",
    "        return r.text, meta\n",
    "    except Exception as e:\n",
    "        meta[\"error\"] = repr(e)\n",
    "        return None, meta\n",
    "\n",
    "def _stable_id(url: str, text: str) -> str:\n",
    "    h = hashlib.sha1((url + \"\n",
    "\" + (text or \"\")[:4000]).encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:16]\n",
    "    return f\"web_{h}\"\n",
    "\n",
    "def parse_websites(\n",
    "    urls: List[str],\n",
    "    *,\n",
    "    selector: Optional[str] = None,\n",
    "    min_chars: int = 400,\n",
    "    cfg: Optional[FetchConfig] = None,\n",
    ") -> pd.DataFrame:\n",
    "    cfg = cfg or FetchConfig()\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        for url in urls:\n",
    "            url = (url or \"\").strip()\n",
    "            if not url:\n",
    "                continue\n",
    "\n",
    "            html, fetch_meta = _fetch_html(url, cfg, session=session)\n",
    "            if not html:\n",
    "                rows.append({\n",
    "                    \"doc_id\": _stable_id(url, \"\"),\n",
    "                    \"source\": \"website\",\n",
    "                    \"text_raw\": \"\",\n",
    "                    \"date\": None,\n",
    "                    \"url\": url,\n",
    "                    \"meta\": {\"fetch\": fetch_meta},\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "            _strip_noise(soup)\n",
    "\n",
    "            title = _get_title(soup)\n",
    "            date = _get_date(soup)\n",
    "            text_raw, extract_meta = _extract_main_text(soup, selector=selector, min_chars=min_chars)\n",
    "\n",
    "            meta = {\n",
    "                \"fetch\": fetch_meta,\n",
    "                \"title\": title,\n",
    "                \"date_extracted\": date.isoformat() if isinstance(date, pd.Timestamp) else None,\n",
    "                \"extraction\": extract_meta,\n",
    "                \"domain\": urlparse(fetch_meta.get(\"final_url\") or url).netloc,\n",
    "            }\n",
    "\n",
    "            rows.append({\n",
    "                \"doc_id\": _stable_id(fetch_meta.get(\"final_url\") or url, text_raw),\n",
    "                \"source\": \"website\",\n",
    "                \"text_raw\": text_raw,\n",
    "                \"date\": date,\n",
    "                \"url\": fetch_meta.get(\"final_url\") or url,\n",
    "                \"meta\": meta,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame(columns=[\"doc_id\", \"source\", \"text_raw\", \"date\", \"url\", \"meta\"])\n",
    "\n",
    "    df[\"text_clean\"] = df[\"text_raw\"].map(clean_text_minimal)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7386cf",
   "metadata": {},
   "source": [
    "\n",
    "## Помощники: чтение XLSX и VK через SOIKA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394c010",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_links_from_upload(upload: widgets.FileUpload) -> List[str]:\n",
    "    if not upload.value:\n",
    "        return []\n",
    "    _, file_info = next(iter(upload.value.items()))\n",
    "    content = file_info.get(\"content\")\n",
    "    if content is None:\n",
    "        return []\n",
    "    try:\n",
    "        df = pd.read_excel(io.BytesIO(content))\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    for col in [\"link\", \"links\", \"url\", \"urls\", \"group\", \"vk\", \"page\"]:\n",
    "        if col in df.columns:\n",
    "            series = df[col]\n",
    "            break\n",
    "    else:\n",
    "        series = df.iloc[:, 0]\n",
    "    return [str(x).strip() for x in series.dropna().astype(str).tolist() if str(x).strip()]\n",
    "\n",
    "def normalize_vk_domains(items: List[str]) -> List[str]:\n",
    "    normalized = []\n",
    "    for raw in items:\n",
    "        raw = (raw or \"\").strip()\n",
    "        if not raw:\n",
    "            continue\n",
    "        raw = raw.lstrip(\"@\").replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "        if raw.startswith(\"vk.com/\"):\n",
    "            raw = raw.split(\"vk.com/\")[-1]\n",
    "        if \"/\" in raw:\n",
    "            raw = raw.split(\"/\")[0]\n",
    "        normalized.append(raw)\n",
    "    return normalized\n",
    "\n",
    "def collect_vk_soika(groups: List[str], token: str, cutoff: Optional[pd.Timestamp], limit: int = 500) -> pd.DataFrame:\n",
    "    if VKParser is None:\n",
    "        raise ImportError(_soika_import_error or \"SOIKA не установлена\")\n",
    "    cutoff_str = cutoff.strftime(\"%Y-%m-%d\") if cutoff is not None else \"1970-01-01\"\n",
    "    frames = []\n",
    "    for group in groups:\n",
    "        df_raw = VKParser.run_parser(domain=group, access_token=token, cutoff_date=cutoff_str, number_of_messages=limit)\n",
    "        if df_raw is None or df_raw.empty:\n",
    "            continue\n",
    "        df = pd.DataFrame(df_raw)\n",
    "        df = df.rename(columns={\"text\": \"text_raw\", \"date\": \"date_raw\"})\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date_raw\"], errors=\"coerce\")\n",
    "        df[\"doc_id\"] = df.apply(lambda r: f\"vk_{group}_{r.get('id')}\", axis=1)\n",
    "        df[\"url\"] = df.apply(lambda r: f\"https://vk.com/wall{r.get('from_id', '')}_{r.get('id', '')}\", axis=1)\n",
    "        df[\"meta\"] = df.apply(lambda r: {\n",
    "            \"group\": group,\n",
    "            \"type\": r.get(\"type\"),\n",
    "            \"likes\": r.get(\"likes.count\"),\n",
    "            \"reposts\": r.get(\"reposts.count\"),\n",
    "            \"views\": r.get(\"views.count\"),\n",
    "            \"link\": r.get(\"link\"),\n",
    "        }, axis=1)\n",
    "        frames.append(df[[\"doc_id\", \"text_raw\", \"date\", \"url\", \"meta\"]])\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=[\"doc_id\", \"text_raw\", \"date\", \"url\", \"meta\"])\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "    out[\"source\"] = \"vk\"\n",
    "    out[\"text_clean\"] = out[\"text_raw\"].map(clean_text_minimal)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5151b493",
   "metadata": {},
   "source": [
    "\n",
    "### Отдельный UI для парсинга сайтов\n",
    "1) Вставьте URL-ы по одному на строку или загрузите XLSX со ссылками.\n",
    "2) (Опционально) укажите CSS-селектор (например `article`, `.post-content`, `main`).\n",
    "3) Нажмите **Собрать тексты**. Можно просмотреть первый текст или скачать CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0da12",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "web_urls_ta = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder=\"https://example.com/article1\n",
    "https://example.com/article2\",\n",
    "    description=\"URL-ы:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"900px\", height=\"160px\"),\n",
    ")\n",
    "\n",
    "web_links_upload = widgets.FileUpload(\n",
    "    accept=\".xlsx\",\n",
    "    multiple=False,\n",
    "    description=\"XLSX со ссылками\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "\n",
    "web_selector_txt = widgets.Text(\n",
    "    value=\"\",\n",
    "    placeholder=\"(опционально) например: article или .post-content\",\n",
    "    description=\"CSS селектор:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"560px\"),\n",
    ")\n",
    "\n",
    "web_min_chars_int = widgets.IntSlider(\n",
    "    value=400, min=100, max=4000, step=50,\n",
    "    description=\"Мин. длина текста:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"560px\"),\n",
    ")\n",
    "\n",
    "web_timeout_int = widgets.IntSlider(\n",
    "    value=20, min=5, max=120, step=5,\n",
    "    description=\"Таймаут (сек):\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout=widgets.Layout(width=\"560px\"),\n",
    ")\n",
    "\n",
    "web_parse_btn = widgets.Button(description=\"Собрать тексты\", button_style=\"primary\", icon=\"play\")\n",
    "web_demo_btn = widgets.Button(description=\"Подставить DEMO URL\", button_style=\"info\")\n",
    "web_save_btn = widgets.Button(description=\"Скачать CSV\", button_style=\"success\", icon=\"download\")\n",
    "web_preview_btn = widgets.Button(description=\"Показать 1-й текст\", button_style=\"\")\n",
    "\n",
    "web_status_out = widgets.Output()\n",
    "web_table_out = widgets.Output()\n",
    "web_preview_out = widgets.Output()\n",
    "\n",
    "web_docs: Optional[pd.DataFrame] = None\n",
    "\n",
    "def _web_get_urls() -> List[str]:\n",
    "    urls = [u.strip() for u in web_urls_ta.value.splitlines() if u.strip()]\n",
    "    urls.extend(read_links_from_upload(web_links_upload))\n",
    "    return urls\n",
    "\n",
    "def _web_set_demo(_=None):\n",
    "    web_urls_ta.value = \"\"\"https://example.com\n",
    "https://www.iana.org/domains/reserved\"\"\"\n",
    "\n",
    "\n",
    "def _web_parse(_=None):\n",
    "    global web_docs\n",
    "    urls = _web_get_urls()\n",
    "    cfg = FetchConfig(timeout=int(web_timeout_int.value))\n",
    "    selector = web_selector_txt.value.strip() or None\n",
    "    min_chars = int(web_min_chars_int.value)\n",
    "\n",
    "    with web_status_out:\n",
    "        web_status_out.clear_output()\n",
    "        display(Markdown(\n",
    "            f\"**URL-ов:** {len(urls)}  \\n\" +\n",
    "            f\"**selector:** `{selector}`  \\n\" +\n",
    "            f\"**min_chars:** {min_chars}  \\n\" +\n",
    "            f\"**timeout:** {cfg.timeout}s\"\n",
    "        ))\n",
    "\n",
    "    with web_table_out:\n",
    "        web_table_out.clear_output()\n",
    "        if not urls:\n",
    "            display(Markdown(\"⚠️ Список URL пуст.\"))\n",
    "            return\n",
    "        web_docs = parse_websites(urls, selector=selector, min_chars=min_chars, cfg=cfg)\n",
    "        display(Markdown(f\"✅ Готово. Документов: **{len(web_docs)}**\"))\n",
    "        display(web_docs[[\"doc_id\", \"source\", \"date\", \"url\"]].head(50))\n",
    "\n",
    "\n",
    "def _web_preview(_=None):\n",
    "    with web_preview_out:\n",
    "        web_preview_out.clear_output()\n",
    "        if web_docs is None or web_docs.empty:\n",
    "            display(Markdown(\"⚠️ Сначала нажми **Собрать тексты**.\"))\n",
    "            return\n",
    "        row = web_docs.iloc[0].to_dict()\n",
    "        title = (row.get('meta') or {}).get('title') or 'Без заголовка'\n",
    "        display(Markdown(f\"### {title}\"))\n",
    "        display(Markdown(f\"**URL:** {row.get('url')}\"))\n",
    "        txt = (row.get(\"text_raw\") or \"\").strip()\n",
    "        if not txt:\n",
    "            display(Markdown(\"(текст не извлечён — см. `meta.fetch.error`)\"))\n",
    "        else:\n",
    "            display(Markdown(txt[:2500] + (\"…\" if len(txt) > 2500 else \"\")))\n",
    "\n",
    "\n",
    "def _web_save(_=None):\n",
    "    if web_docs is None or web_docs.empty:\n",
    "        with web_status_out:\n",
    "            web_status_out.clear_output()\n",
    "            display(Markdown(\"⚠️ Нечего сохранять. Сначала нажми **Собрать тексты**.\"))\n",
    "            return\n",
    "    path = \"docs_web.csv\"\n",
    "    web_docs.to_csv(path, index=False, encoding=\"utf-8\")\n",
    "    with web_status_out:\n",
    "        web_status_out.clear_output()\n",
    "        display(Markdown(f\"✅ Сохранено в файл: `{path}` (скачай через файловый браузер Jupyter/Colab).\"))\n",
    "\n",
    "web_demo_btn.on_click(_web_set_demo)\n",
    "web_parse_btn.on_click(_web_parse)\n",
    "web_preview_btn.on_click(_web_preview)\n",
    "web_save_btn.on_click(_web_save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "main_controls = widgets.VBox([\n",
    "    source_dd,\n",
    "    upload_box,\n",
    "    widgets.HBox([since_txt, until_txt]),\n",
    "    vk_opts,\n",
    "    website_opts,\n",
    "    widgets.HBox([demo_btn, run_btn]),\n",
    "    out,\n",
    "])\n",
    "\n",
    "web_tools_box = widgets.VBox([\n",
    "    widgets.HBox([web_links_upload]),\n",
    "    web_urls_ta,\n",
    "    widgets.HBox([web_demo_btn, web_parse_btn, web_preview_btn, web_save_btn]),\n",
    "    widgets.HBox([web_selector_txt, web_min_chars_int]),\n",
    "    web_timeout_int,\n",
    "    web_status_out,\n",
    "    web_table_out,\n",
    "    web_preview_out,\n",
    "])\n",
    "\n",
    "advanced_web = widgets.Accordion(children=[web_tools_box])\n",
    "advanced_web.set_title(0, \"Расширенный веб-парсер\")\n",
    "\n",
    "ui = widgets.VBox([main_controls, advanced_web])\n",
    "display(ui)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aa0597",
   "metadata": {},
   "source": [
    "\n",
    "## Stub-функции источников\n",
    "Здесь **точки подключения** к реальным парсерам.\n",
    "\n",
    "- VK: если установлен SOIKA и указан токен, используется реальный парсер `VKParser.run_parser`; иначе остаётся DEMO-заглушка.\n",
    "- Сайты и Яндекс Отзывы — реальный парсер сайтов + простая заглушка для Яндекс (нужны ссылки).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56175be4",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "SPACE_RE = re.compile(r\"\\s+\")\n",
    "PUNCT_RUN_RE = re.compile(r\"([!?.,])\u0001{2,}\")\n",
    "\n",
    "def collect_vk_stub(group_ids: List[str], since: Optional[pd.Timestamp], until: Optional[pd.Timestamp]) -> pd.DataFrame:\n",
    "    rows = [\n",
    "        {\n",
    "            \"doc_id\": \"vk_1\",\n",
    "            \"source\": \"vk\",\n",
    "            \"text_raw\": \"Люблю этот район — здесь тихо и много зелени. Но парковки не хватает!!!\",\n",
    "            \"date\": pd.Timestamp(\"2025-09-12\"),\n",
    "            \"url\": None,\n",
    "            \"meta\": {\"group_id\": group_ids[0] if group_ids else None},\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"vk_2\",\n",
    "            \"source\": \"vk\",\n",
    "            \"text_raw\": \"Опять перекопали улицу у станции. Дойти до остановки — квест.\",\n",
    "            \"date\": pd.Timestamp(\"2025-10-03\"),\n",
    "            \"url\": None,\n",
    "            \"meta\": {\"group_id\": group_ids[0] if group_ids else None},\n",
    "        },\n",
    "    ]\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def collect_websites_stub(urls: List[str], selector: Optional[str], min_chars: int, timeout: int) -> pd.DataFrame:\n",
    "    if urls:\n",
    "        return parse_websites(urls, selector=selector, min_chars=min_chars, cfg=FetchConfig(timeout=timeout))\n",
    "    rows = [\n",
    "        {\n",
    "            \"doc_id\": \"web_1\",\n",
    "            \"source\": \"website\",\n",
    "            \"text_raw\": \"<article>Исторический квартал меняется: появляются новые кафе и мастерские.</article>\",\n",
    "            \"date\": None,\n",
    "            \"url\": urls[0] if urls else None,\n",
    "            \"meta\": {\"title\": \"Заглушка статьи\"},\n",
    "        }\n",
    "    ]\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"text_clean\"] = df[\"text_raw\"].map(clean_text_minimal)\n",
    "    return df\n",
    "\n",
    "def collect_yandex_reviews_stub(urls: List[str]) -> pd.DataFrame:\n",
    "    rows = [\n",
    "        {\n",
    "            \"doc_id\": \"ya_1\",\n",
    "            \"source\": \"yandex_reviews\",\n",
    "            \"text_raw\": \"Удобно добираться, но внутри тесно. Персонал норм.\",\n",
    "            \"date\": None,\n",
    "            \"url\": urls[0] if urls else None,\n",
    "            \"meta\": {\"rating\": 3, \"place\": \"Заглушка\"},\n",
    "        }\n",
    "    ]\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"text_clean\"] = df[\"text_raw\"].map(clean_text_minimal)\n",
    "    return df\n",
    "\n",
    "def standardize_docs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    required_cols = [\"doc_id\", \"source\", \"text_raw\", \"date\", \"url\", \"meta\"]\n",
    "    for c in required_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "    df = df[required_cols].copy()\n",
    "    df[\"text_clean\"] = df[\"text_raw\"].map(clean_text_minimal)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a132d6b0",
   "metadata": {},
   "source": [
    "\n",
    "## Запуск (DEMO / SOIKA)\n",
    "Ниже — обработчики кнопок. Можно загрузить DEMO или выполнить реальный сбор. Для VK укажите токен и домены групп (или ссылки на них). Для сайтов и Яндекс Отзывов — передайте ссылки вручную или через XLSX.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9556a67d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "docs: Optional[pd.DataFrame] = None\n",
    "\n",
    "\n",
    "def get_inputs() -> Dict[str, Any]:\n",
    "    src = source_dd.value\n",
    "    manual_raw = input_ta.value.strip()\n",
    "    single = single_link_txt.value.strip()\n",
    "    since = parse_date_safe(since_txt.value)\n",
    "    until = parse_date_safe(until_txt.value)\n",
    "\n",
    "    manual_items: List[str]\n",
    "    if src == \"vk\":\n",
    "        manual_items = [x.strip() for x in re.split(r\"[\n",
    ",;]+\", manual_raw) if x.strip()]\n",
    "    else:\n",
    "        manual_items = [x.strip() for x in manual_raw.splitlines() if x.strip()]\n",
    "\n",
    "    items = manual_items\n",
    "    if single:\n",
    "        items.append(single)\n",
    "    items.extend(read_links_from_upload(links_upload))\n",
    "\n",
    "    return {\"source\": src, \"items\": items, \"since\": since, \"until\": until}\n",
    "\n",
    "\n",
    "def load_demo(_=None):\n",
    "    global docs\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        demo = pd.concat([\n",
    "            collect_vk_stub([\"demo_group\"], None, None),\n",
    "            collect_websites_stub([\"https://example.com/article\"], selector_main.value or None, min_chars_main.value, timeout_main.value),\n",
    "            collect_yandex_reviews_stub([\"https://example.com/reviews\"]),\n",
    "        ], ignore_index=True)\n",
    "        docs = standardize_docs(demo)\n",
    "        display(Markdown(\"✅ Загружен DEMO-корпус. Ниже — первые строки `docs`.\"))\n",
    "        display(docs.head(10))\n",
    "\n",
    "\n",
    "def run_pipeline(_=None):\n",
    "    global docs\n",
    "    cfg = get_inputs()\n",
    "    src = cfg[\"source\"]\n",
    "    items = cfg[\"items\"]\n",
    "\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        display(Markdown(\n",
    "            f\"**Источник:** `{src}`  \\n\" +\n",
    "            f\"**Элементы ввода:** {len(items)}  \\n\" +\n",
    "            f\"**Период:** {cfg['since']} — {cfg['until']}\"\n",
    "        ))\n",
    "\n",
    "        if src == \"vk\":\n",
    "            groups = normalize_vk_domains(items)\n",
    "            if VKParser is not None and vk_token_txt.value:\n",
    "                try:\n",
    "                    df = collect_vk_soika(groups, vk_token_txt.value, cfg[\"since\"])\n",
    "                except Exception as e:\n",
    "                    display(Markdown(f\"⚠️ Ошибка SOIKA: `{e}`. Использую DEMO.\"))\n",
    "                    df = collect_vk_stub(groups, cfg[\"since\"], cfg[\"until\"])\n",
    "            else:\n",
    "                if VKParser is None:\n",
    "                    display(Markdown(\"⚠️ SOIKA недоступна (проверь установку и Python<3.12). Использую DEMO.\"))\n",
    "                elif not vk_token_txt.value:\n",
    "                    display(Markdown(\"⚠️ Укажи VK токен для реального парсинга. Пока DEMO.\"))\n",
    "                df = collect_vk_stub(groups, cfg[\"since\"], cfg[\"until\"])\n",
    "        elif src == \"website\":\n",
    "            df = collect_websites_stub(items, selector_main.value or None, int(min_chars_main.value), int(timeout_main.value))\n",
    "        else:\n",
    "            df = collect_yandex_reviews_stub(items)\n",
    "\n",
    "        docs = standardize_docs(df)\n",
    "        display(Markdown(\"✅ Получена таблица `docs` (в формате для дальнейших разделов).\"))\n",
    "        display(docs)\n",
    "\n",
    "\n",
    "demo_btn.on_click(load_demo)\n",
    "run_btn.on_click(run_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b840bb",
   "metadata": {},
   "source": [
    "## Быстрый контроль качества корпуса (минимальный)\n",
    "Это не “полный препроцессинг”, а sanity-check:\n",
    "- сколько документов,\n",
    "- распределение длины,\n",
    "- топ-слова (очень грубо)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daca2ad",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def quick_qc(docs: pd.DataFrame) -> None:\n",
    "    if docs is None or docs.empty:\n",
    "        display(Markdown(\"⚠️ `docs` пустой. Сначала загрузите DEMO или выполните сбор.\"))\n",
    "        return\n",
    "\n",
    "    df = docs.copy()\n",
    "    df[\"len_chars\"] = df[\"text_clean\"].map(lambda s: len(s or \"\"))\n",
    "    df[\"len_words\"] = df[\"text_clean\"].map(lambda s: len((s or \"\").split()))\n",
    "\n",
    "    display(Markdown(\"### Сводка\"))\n",
    "    display(df.groupby(\"source\").agg(\n",
    "        n_docs=(\"doc_id\", \"count\"),\n",
    "        avg_words=(\"len_words\", \"mean\"),\n",
    "        p50_words=(\"len_words\", \"median\"),\n",
    "        avg_chars=(\"len_chars\", \"mean\"),\n",
    "    ))\n",
    "\n",
    "    display(Markdown(\"### Длина текста (слов)\"))\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    df[\"len_words\"].hist(bins=20)\n",
    "    plt.xlabel(\"words\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "    display(Markdown(\"### Топ-слов (очень грубо)\"))\n",
    "    token_re = re.compile(r\"[а-яё]+\", re.IGNORECASE)\n",
    "    tokens = []\n",
    "    for t in df[\"text_clean\"].astype(str).tolist():\n",
    "        tokens.extend(token_re.findall(t.lower()))\n",
    "    if not tokens:\n",
    "        display(Markdown(\"(нет токенов после очистки)\"))\n",
    "        return\n",
    "    vc = pd.Series(tokens).value_counts().head(25)\n",
    "    display(vc.to_frame(\"count\"))\n",
    "\n",
    "if docs is not None:\n",
    "    quick_qc(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493c92b",
   "metadata": {},
   "source": [
    "\n",
    "## Что дальше\n",
    "Следующий раздел книги: **«Подготовка корпуса»** — там будет:\n",
    "- более строгая очистка,\n",
    "- дедупликация,\n",
    "- подготовка к эмбеддингам и тематизации.\n",
    "\n",
    "> Чтобы запустить VK-парсер, убедись, что `pip install \"soika\"` выполнен в среде Python 3.11 и ниже, затем укажи токен.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}