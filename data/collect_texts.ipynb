{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Краткое описание\n",
    "\n",
    "Блокнот собирает тексты из VK, сайтов и Яндекс Отзывов и приводит их к единому формату `docs`. Запуск выполняется через UI в следующей ячейке.  \n",
    "Исходный код расположен в скрытой ячейке, которую можно раскрыть при необходимости.\n",
    "\n",
    "## Как получить токены\n",
    "\n",
    "### VK (SOIKA)\n",
    "1. Откройте https://dev.vk.com/ и создайте приложение (тип — Standalone).\n",
    "2. В разделе **Авторизация** получите пользовательский `access_token` с правами `wall`, `groups`, `offline`.\n",
    "3. Вставьте токен в поле **VK токен** в форме ниже.\n",
    "\n",
    "### Яндекс Отзывы\n",
    "1. Перейдите в https://developer.tech.yandex.ru/ и создайте API-ключ для сервиса **Яндекс Отзывы / Business Reviews API** (если доступ ограничен — запросите доступ в кабинете разработчика).\n",
    "2. Скопируйте ключ и вставьте его в поле **Yandex токен**.\n",
    "3. Возьмите ID организации из URL Яндекс Карт вида `https://yandex.ru/maps/org/<название>/<ID>/reviews/` или передайте сам ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import importlib\n",
    "import importlib.util\n",
    "import io\n",
    "import itertools\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "_vk_spec = importlib.util.find_spec(\"soika\")\n",
    "if _vk_spec is None:\n",
    "    VKParser = None  # type: ignore\n",
    "    _soika_import_error = \"SOIKA не установлена\"\n",
    "else:\n",
    "    soika = importlib.import_module(\"soika\")\n",
    "    VKParser = getattr(soika, \"VKParser\", None)\n",
    "    _soika_import_error = None if VKParser is not None else \"В пакете soika нет VKParser\"\n",
    "\n",
    "_is_colab = importlib.util.find_spec(\"google.colab\") is not None\n",
    "if _is_colab:\n",
    "    from google.colab import output  # type: ignore\n",
    "\n",
    "    output.enable_custom_widget_manager()\n",
    "\n",
    "HTML_TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "SPACE_RE = re.compile(r\"\\s+\")\n",
    "PUNCT_RUN_RE = re.compile(r\"([!?.,])\\1{2,}\")\n",
    "\n",
    "\n",
    "def clean_text_minimal(text: str) -> str:\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    t = str(text)\n",
    "    t = HTML_TAG_RE.sub(\" \", t)\n",
    "    t = PUNCT_RUN_RE.sub(r\"\\1\\1\", t)\n",
    "    t = SPACE_RE.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "\n",
    "def parse_date_safe(s: str) -> Optional[pd.Timestamp]:\n",
    "    s = (s or \"\").strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    try:\n",
    "        return pd.to_datetime(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Извлечение содержимого из HTML\n",
    "# ----------------------------\n",
    "\n",
    "def _strip_noise(soup: BeautifulSoup) -> None:\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\", \"canvas\", \"iframe\"]):\n",
    "        tag.decompose()\n",
    "    for tag in soup.find_all([\"nav\", \"footer\", \"header\", \"aside\"]):\n",
    "        tag.decompose()\n",
    "    for c in soup.find_all(string=lambda x: isinstance(x, Comment)):\n",
    "        c.extract()\n",
    "\n",
    "\n",
    "def _get_title(soup: BeautifulSoup) -> Optional[str]:\n",
    "    for attr, val in [(\"property\", \"og:title\"), (\"name\", \"twitter:title\")]:\n",
    "        node = soup.find(\"meta\", attrs={attr: val})\n",
    "        if node and node.get(\"content\"):\n",
    "            return clean_text_minimal(node[\"content\"])\n",
    "\n",
    "    if soup.title and soup.title.get_text(strip=True):\n",
    "        return clean_text_minimal(soup.title.get_text(\" \", strip=True))\n",
    "\n",
    "    h1 = soup.find(\"h1\")\n",
    "    if h1 and h1.get_text(strip=True):\n",
    "        return clean_text_minimal(h1.get_text(\" \", strip=True))\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _safe_to_datetime(value: Optional[str]) -> Optional[pd.Timestamp]:\n",
    "    if not value:\n",
    "        return None\n",
    "    dt = pd.to_datetime(value, errors=\"coerce\", utc=True)\n",
    "    if pd.isna(dt):\n",
    "        return None\n",
    "    return dt\n",
    "\n",
    "\n",
    "def _get_date(soup: BeautifulSoup) -> Optional[pd.Timestamp]:\n",
    "    meta_candidates = [\n",
    "        (\"property\", \"article:published_time\"),\n",
    "        (\"property\", \"article:modified_time\"),\n",
    "        (\"property\", \"og:updated_time\"),\n",
    "        (\"name\", \"pubdate\"),\n",
    "        (\"name\", \"publishdate\"),\n",
    "        (\"name\", \"timestamp\"),\n",
    "        (\"name\", \"date\"),\n",
    "        (\"name\", \"DC.date.issued\"),\n",
    "        (\"name\", \"DC.Date\"),\n",
    "        (\"itemprop\", \"datePublished\"),\n",
    "        (\"itemprop\", \"dateModified\"),\n",
    "    ]\n",
    "    for attr, val in meta_candidates:\n",
    "        node = soup.find(\"meta\", attrs={attr: val})\n",
    "        if node and node.get(\"content\"):\n",
    "            dt = _safe_to_datetime(node[\"content\"])\n",
    "            if dt is not None:\n",
    "                return dt\n",
    "\n",
    "    time_tag = soup.find(\"time\")\n",
    "    if time_tag:\n",
    "        dt = _safe_to_datetime(time_tag.get(\"datetime\"))\n",
    "        if dt is not None:\n",
    "            return dt\n",
    "    return None\n",
    "\n",
    "\n",
    "def _node_text_len(node) -> int:\n",
    "    if not hasattr(node, \"get_text\"):\n",
    "        return 0\n",
    "    return len(node.get_text(\" \", strip=True))\n",
    "\n",
    "\n",
    "def _extract_main_text(\n",
    "    soup: BeautifulSoup, *, selector: Optional[str] = None, min_chars: int = 400\n",
    ") -> Tuple[str, Dict[str, Any]]:\n",
    "    meta: Dict[str, Any] = {\"extractor\": None}\n",
    "\n",
    "    if selector:\n",
    "        nodes = soup.select(selector)\n",
    "        if nodes:\n",
    "            parts = [n.get_text(\"\\n\", strip=True) for n in nodes]\n",
    "            text = clean_text_minimal(\"\\n\".join(parts))\n",
    "            meta[\"extractor\"] = f\"css:{selector}\"\n",
    "            if len(text) >= min_chars:\n",
    "                return text, meta\n",
    "            meta[\"extractor_fallback\"] = \"too_short\"\n",
    "\n",
    "    for tag_name in [\"article\", \"main\"]:\n",
    "        node = soup.find(tag_name)\n",
    "        if node:\n",
    "            text = clean_text_minimal(node.get_text(\"\\n\", strip=True))\n",
    "            if len(text) >= min_chars:\n",
    "                meta[\"extractor\"] = tag_name\n",
    "                return text, meta\n",
    "\n",
    "    candidates = []\n",
    "    for key in [\"content\", \"article\", \"post\", \"entry\", \"text\", \"body\", \"main\"]:\n",
    "        candidates.extend(soup.find_all(attrs={\"class\": re.compile(key, re.I)}))\n",
    "        candidates.extend(soup.find_all(attrs={\"id\": re.compile(key, re.I)}))\n",
    "\n",
    "    candidates.extend(soup.find_all([\"div\", \"section\"]))\n",
    "\n",
    "    best = None\n",
    "    best_len = 0\n",
    "    for node in candidates:\n",
    "        l = _node_text_len(node)\n",
    "        if l > best_len:\n",
    "            best = node\n",
    "            best_len = l\n",
    "\n",
    "    if best is not None and best_len > 0:\n",
    "        meta[\"extractor\"] = \"largest_block\"\n",
    "        text = clean_text_minimal(best.get_text(\"\\n\", strip=True))\n",
    "        return text, meta\n",
    "\n",
    "    meta[\"extractor\"] = \"none\"\n",
    "    return \"\", meta\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# HTTP helpers\n",
    "# ----------------------------\n",
    "\n",
    "@dataclass\n",
    "class FetchConfig:\n",
    "    timeout: int = 20\n",
    "    user_agent: str = (\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari\"\n",
    "    )\n",
    "    max_bytes: int = 5_000_000  # 5MB\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class YandexReviewsConfig:\n",
    "    api_base: str = \"https://api-maps.yandex.ru/v3/businesses\"\n",
    "    timeout: int = 20\n",
    "    user_agent: str = (\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari\"\n",
    "    )\n",
    "    per_page: int = 50\n",
    "    max_pages: int = 200\n",
    "\n",
    "\n",
    "YANDEX_ORG_ID_RE = re.compile(r\"(?:org/[^/]+/)?(\\d{5,})\")\n",
    "\n",
    "\n",
    "def extract_yandex_org_id(raw: str) -> Optional[str]:\n",
    "    raw = (raw or \"\").strip()\n",
    "    if not raw:\n",
    "        return None\n",
    "    if raw.isdigit():\n",
    "        return raw\n",
    "    match = YANDEX_ORG_ID_RE.search(raw)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    match = re.search(r\"\\b(\\d{5,})\\b\", raw)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "def normalize_yandex_org_ids(items: List[str]) -> List[str]:\n",
    "    org_ids: List[str] = []\n",
    "    for item in items:\n",
    "        org_id = extract_yandex_org_id(item)\n",
    "        if org_id and org_id not in org_ids:\n",
    "            org_ids.append(org_id)\n",
    "    return org_ids\n",
    "\n",
    "\n",
    "def _first_value(payload: Dict[str, Any], keys: List[str]) -> Optional[Any]:\n",
    "    for key in keys:\n",
    "        if key in payload and payload[key] not in (None, \"\"):\n",
    "            return payload[key]\n",
    "    return None\n",
    "\n",
    "\n",
    "def _extract_review_items(payload: Any) -> List[Dict[str, Any]]:\n",
    "    if isinstance(payload, list):\n",
    "        return [p for p in payload if isinstance(p, dict)]\n",
    "    if not isinstance(payload, dict):\n",
    "        return []\n",
    "    for key in [\"reviews\", \"items\", \"data\", \"result\"]:\n",
    "        value = payload.get(key)\n",
    "        if isinstance(value, list):\n",
    "            return [p for p in value if isinstance(p, dict)]\n",
    "        if isinstance(value, dict):\n",
    "            for subkey in [\"reviews\", \"items\", \"data\", \"result\"]:\n",
    "                sub = value.get(subkey)\n",
    "                if isinstance(sub, list):\n",
    "                    return [p for p in sub if isinstance(p, dict)]\n",
    "    return []\n",
    "\n",
    "\n",
    "def _extract_next_page(payload: Any) -> Optional[str]:\n",
    "    if not isinstance(payload, dict):\n",
    "        return None\n",
    "    for key in [\"next_page_token\", \"nextPageToken\", \"next_page\", \"next\"]:\n",
    "        val = payload.get(key)\n",
    "        if isinstance(val, str) and val:\n",
    "            return val\n",
    "    pagination = payload.get(\"pagination\")\n",
    "    if isinstance(pagination, dict):\n",
    "        for key in [\"next_page_token\", \"nextPageToken\", \"next_page\", \"next\"]:\n",
    "            val = pagination.get(key)\n",
    "            if isinstance(val, str) and val:\n",
    "                return val\n",
    "    return None\n",
    "\n",
    "\n",
    "def collect_yandex_reviews(\n",
    "    org_ids: List[str],\n",
    "    token: str,\n",
    "    *,\n",
    "    limit: int = 300,\n",
    "    cfg: Optional[YandexReviewsConfig] = None,\n",
    ") -> pd.DataFrame:\n",
    "    cfg = cfg or YandexReviewsConfig()\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    headers = {\"User-Agent\": cfg.user_agent, \"Authorization\": f\"Api-Key {token}\"}\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        for org_id in org_ids:\n",
    "            fetched = 0\n",
    "            page_count = 0\n",
    "            next_token: Optional[str] = None\n",
    "            offset = 0\n",
    "\n",
    "            while fetched < limit and page_count < cfg.max_pages:\n",
    "                page_size = min(cfg.per_page, limit - fetched)\n",
    "                params: Dict[str, Any] = {\n",
    "                    \"apikey\": token,\n",
    "                    \"lang\": \"ru_RU\",\n",
    "                    \"limit\": page_size,\n",
    "                    \"offset\": offset,\n",
    "                    \"sort\": \"date\",\n",
    "                }\n",
    "                if next_token:\n",
    "                    params.pop(\"offset\", None)\n",
    "                    params[\"page_token\"] = next_token\n",
    "\n",
    "                url = f\"{cfg.api_base}/{org_id}/reviews\"\n",
    "                response = session.get(url, headers=headers, params=params, timeout=cfg.timeout)\n",
    "                if response.status_code >= 400:\n",
    "                    raise RuntimeError(f\"Yandex Reviews API error {response.status_code}: {response.text[:200]}\")\n",
    "\n",
    "                payload = response.json()\n",
    "                items = _extract_review_items(payload)\n",
    "                if not items:\n",
    "                    break\n",
    "\n",
    "                for item in items:\n",
    "                    text = _first_value(item, [\"text\", \"comment\", \"description\", \"body\"]) or \"\"\n",
    "                    rating = _first_value(item, [\"rating\", \"score\", \"stars\"])\n",
    "                    date_raw = _first_value(item, [\"created_at\", \"created\", \"date\", \"updated_at\"])\n",
    "                    author = _first_value(item, [\"author\", \"user\", \"name\"])\n",
    "                    likes = _first_value(item, [\"likes\", \"likes_count\", \"useful\"])\n",
    "                    review_id = _first_value(item, [\"id\", \"review_id\"])\n",
    "                    url_value = _first_value(item, [\"url\", \"link\"])\n",
    "\n",
    "                    rows.append(\n",
    "                        {\n",
    "                            \"doc_id\": f\"ya_{org_id}_{review_id or len(rows)}\",\n",
    "                            \"source\": \"yandex_reviews\",\n",
    "                            \"text_raw\": text,\n",
    "                            \"date\": parse_date_safe(str(date_raw)) if date_raw is not None else None,\n",
    "                            \"url\": url_value or f\"https://yandex.ru/maps/org/{org_id}/reviews/\",\n",
    "                            \"meta\": {\n",
    "                                \"org_id\": org_id,\n",
    "                                \"rating\": rating,\n",
    "                                \"author\": author,\n",
    "                                \"likes\": likes,\n",
    "                                \"review_id\": review_id,\n",
    "                            },\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                fetched += len(items)\n",
    "                page_count += 1\n",
    "                offset += len(items)\n",
    "                next_token = _extract_next_page(payload)\n",
    "                if not next_token and len(items) < page_size:\n",
    "                    break\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame(columns=[\"doc_id\", \"source\", \"text_raw\", \"date\", \"url\", \"meta\"])\n",
    "    df[\"text_clean\"] = df[\"text_raw\"].map(clean_text_minimal)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def _fetch_html(url: str, cfg: FetchConfig, session: requests.Session) -> Tuple[Optional[str], Dict[str, Any]]:\n",
    "    meta: Dict[str, Any] = {\"status_code\": None, \"final_url\": None}\n",
    "    try:\n",
    "        r = session.get(\n",
    "            url,\n",
    "            headers={\"User-Agent\": cfg.user_agent, \"Accept\": \"text/html,application/xhtml+xml\"},\n",
    "            timeout=cfg.timeout,\n",
    "            allow_redirects=True,\n",
    "        )\n",
    "        meta[\"status_code\"] = r.status_code\n",
    "        meta[\"final_url\"] = r.url\n",
    "        r.raise_for_status()\n",
    "\n",
    "        content = r.content\n",
    "        if content and len(content) > cfg.max_bytes:\n",
    "            meta[\"error\"] = f\"response_too_large:{len(content)}\"\n",
    "            return None, meta\n",
    "\n",
    "        if not r.encoding or r.encoding.lower() in {\"iso-8859-1\", \"latin1\", \"ascii\"}:\n",
    "            if r.apparent_encoding:\n",
    "                r.encoding = r.apparent_encoding\n",
    "\n",
    "        return r.text, meta\n",
    "    except Exception as e:\n",
    "        meta[\"error\"] = repr(e)\n",
    "        return None, meta\n",
    "\n",
    "\n",
    "def _stable_id(url: str, text: str) -> str:\n",
    "    h = hashlib.sha1((url + \"\\n\" + (text or \"\")[:4000]).encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:16]\n",
    "    return f\"web_{h}\"\n",
    "\n",
    "\n",
    "def parse_websites(\n",
    "    urls: List[str],\n",
    "    *,\n",
    "    selector: Optional[str] = None,\n",
    "    min_chars: int = 400,\n",
    "    cfg: Optional[FetchConfig] = None,\n",
    ") -> pd.DataFrame:\n",
    "    cfg = cfg or FetchConfig()\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        for url in urls:\n",
    "            url = (url or \"\").strip()\n",
    "            if not url:\n",
    "                continue\n",
    "\n",
    "            html, fetch_meta = _fetch_html(url, cfg, session=session)\n",
    "            if not html:\n",
    "                rows.append({\n",
    "                    \"doc_id\": _stable_id(url, \"\"),\n",
    "                    \"source\": \"website\",\n",
    "                    \"text_raw\": \"\",\n",
    "                    \"date\": None,\n",
    "                    \"url\": url,\n",
    "                    \"meta\": {\"fetch\": fetch_meta},\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "            _strip_noise(soup)\n",
    "\n",
    "            title = _get_title(soup)\n",
    "            date = _get_date(soup)\n",
    "            text_raw, extract_meta = _extract_main_text(soup, selector=selector, min_chars=min_chars)\n",
    "\n",
    "            meta = {\n",
    "                \"fetch\": fetch_meta,\n",
    "                \"title\": title,\n",
    "                \"date_extracted\": date.isoformat() if isinstance(date, pd.Timestamp) else None,\n",
    "                \"extraction\": extract_meta,\n",
    "                \"domain\": urlparse(fetch_meta.get(\"final_url\") or url).netloc,\n",
    "            }\n",
    "\n",
    "            rows.append({\n",
    "                \"doc_id\": _stable_id(fetch_meta.get(\"final_url\") or url, text_raw),\n",
    "                \"source\": \"website\",\n",
    "                \"text_raw\": text_raw,\n",
    "                \"date\": date,\n",
    "                \"url\": fetch_meta.get(\"final_url\") or url,\n",
    "                \"meta\": meta,\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        df = pd.DataFrame(columns=[\"doc_id\", \"source\", \"text_raw\", \"date\", \"url\", \"meta\"])\n",
    "\n",
    "    df[\"text_clean\"] = df[\"text_raw\"].map(clean_text_minimal)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_links_from_upload(upload: widgets.FileUpload) -> List[str]:\n",
    "    if not upload.value:\n",
    "        return []\n",
    "    _, file_info = next(iter(upload.value.items()))\n",
    "    content = file_info.get(\"content\")\n",
    "    if content is None:\n",
    "        return []\n",
    "    try:\n",
    "        df = pd.read_excel(io.BytesIO(content))\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    for col in [\"link\", \"links\", \"url\", \"urls\", \"group\", \"vk\", \"page\"]:\n",
    "        if col in df.columns:\n",
    "            series = df[col]\n",
    "            break\n",
    "    else:\n",
    "        series = df.iloc[:, 0]\n",
    "    return [str(x).strip() for x in series.dropna().astype(str).tolist() if str(x).strip()]\n",
    "\n",
    "\n",
    "def normalize_vk_domains(items: List[str]) -> List[str]:\n",
    "    normalized = []\n",
    "    for raw in items:\n",
    "        raw = (raw or \"\").strip()\n",
    "        if not raw:\n",
    "            continue\n",
    "        raw = raw.lstrip(\"@\").replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "        if raw.startswith(\"vk.com/\"):\n",
    "            raw = raw.split(\"vk.com/\")[-1]\n",
    "        if \"/\" in raw:\n",
    "            raw = raw.split(\"/\")[0]\n",
    "        normalized.append(raw)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def collect_vk_soika(groups: List[str], token: str, cutoff: Optional[pd.Timestamp], limit: int = 500) -> pd.DataFrame:\n",
    "    if VKParser is None:\n",
    "        raise ImportError(_soika_import_error or \"SOIKA не установлена\")\n",
    "    cutoff_str = cutoff.strftime(\"%Y-%m-%d\") if cutoff is not None else \"1970-01-01\"\n",
    "    frames = []\n",
    "    parser = VKParser()\n",
    "    message_id_counter = itertools.count(1)\n",
    "    for group in groups:\n",
    "        df_raw = parser.run_parser(domain=group, access_token=token, cutoff_date=cutoff_str, number_of_messages=limit)\n",
    "        if df_raw is None or df_raw.empty:\n",
    "            continue\n",
    "        df = pd.DataFrame(df_raw)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        df = df.rename(columns={\"text\": \"text_raw\", \"date\": \"date_raw\"})\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date_raw\"], errors=\"coerce\")\n",
    "\n",
    "        df[\"vk_id\"] = df.get(\"id\")\n",
    "        df[\"vk_parent_id\"] = df.get(\"parent_message_id\")\n",
    "        df[\"message_id\"] = [next(message_id_counter) for _ in range(len(df))]\n",
    "        df[\"parent_message_id\"] = None\n",
    "\n",
    "        id_map = {\n",
    "            vk_id: message_id\n",
    "            for vk_id, message_id in zip(df[\"vk_id\"], df[\"message_id\"])\n",
    "            if pd.notna(vk_id)\n",
    "        }\n",
    "        df[\"parent_message_id\"] = df[\"vk_parent_id\"].map(id_map)\n",
    "\n",
    "        df[\"doc_id\"] = df.apply(lambda r: f\"vk_{group}_{r.get('vk_id')}\", axis=1)\n",
    "        df[\"url\"] = df.apply(lambda r: f\"https://vk.com/wall{r.get('from_id', '')}_{r.get('vk_id', '')}\", axis=1)\n",
    "        df[\"meta\"] = df.apply(lambda r: {\n",
    "            \"group\": group,\n",
    "            \"type\": r.get(\"type\"),\n",
    "            \"likes\": r.get(\"likes.count\"),\n",
    "            \"reposts\": r.get(\"reposts.count\"),\n",
    "            \"views\": r.get(\"views.count\"),\n",
    "            \"link\": r.get(\"link\"),\n",
    "            \"vk_id\": r.get(\"vk_id\"),\n",
    "            \"vk_parent_id\": r.get(\"vk_parent_id\"),\n",
    "            \"message_id\": r.get(\"message_id\"),\n",
    "            \"parent_message_id\": r.get(\"parent_message_id\"),\n",
    "        }, axis=1)\n",
    "        frames.append(\n",
    "            df[\n",
    "                [\n",
    "                    \"doc_id\",\n",
    "                    \"text_raw\",\n",
    "                    \"date\",\n",
    "                    \"url\",\n",
    "                    \"meta\",\n",
    "                    \"message_id\",\n",
    "                    \"parent_message_id\",\n",
    "                    \"vk_id\",\n",
    "                    \"vk_parent_id\",\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "    if not frames:\n",
    "        return pd.DataFrame(\n",
    "            columns=[\"doc_id\", \"text_raw\", \"date\", \"url\", \"meta\", \"message_id\", \"parent_message_id\", \"vk_id\", \"vk_parent_id\"]\n",
    "        )\n",
    "    out = pd.concat(frames, ignore_index=True)\n",
    "    out[\"source\"] = \"vk\"\n",
    "    out[\"text_clean\"] = out[\"text_raw\"].map(clean_text_minimal)\n",
    "    return out\n",
    "\n",
    "\n",
    "def collect_vk_stub(group_ids: List[str], since: Optional[pd.Timestamp], until: Optional[pd.Timestamp]) -> pd.DataFrame:\n",
    "    rows = [\n",
    "        {\n",
    "            \"doc_id\": \"vk_1\",\n",
    "            \"source\": \"vk\",\n",
    "            \"text_raw\": \"Люблю этот район — здесь тихо и много зелени. Но парковки не хватает!!!\",\n",
    "            \"date\": pd.Timestamp(\"2025-09-12\"),\n",
    "            \"url\": None,\n",
    "            \"meta\": {\"group_id\": group_ids[0] if group_ids else None},\n",
    "        },\n",
    "        {\n",
    "            \"doc_id\": \"vk_2\",\n",
    "            \"source\": \"vk\",\n",
    "            \"text_raw\": \"Опять перекопали улицу у станции. Дойти до остановки — квест.\",\n",
    "            \"date\": pd.Timestamp(\"2025-10-03\"),\n",
    "            \"url\": None,\n",
    "            \"meta\": {\"group_id\": group_ids[0] if group_ids else None},\n",
    "        },\n",
    "    ]\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def collect_websites_stub(urls: List[str], selector: Optional[str], min_chars: int, timeout: int) -> pd.DataFrame:\n",
    "    if urls:\n",
    "        return parse_websites(urls, selector=selector, min_chars=min_chars, cfg=FetchConfig(timeout=timeout))\n",
    "    rows = [\n",
    "        {\n",
    "            \"doc_id\": \"web_1\",\n",
    "            \"source\": \"website\",\n",
    "            \"text_raw\": \"<article>Исторический квартал меняется: появляются новые кафе и мастерские.</article>\",\n",
    "            \"date\": None,\n",
    "            \"url\": urls[0] if urls else None,\n",
    "            \"meta\": {\"title\": \"Заглушка статьи\"},\n",
    "        }\n",
    "    ]\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"text_clean\"] = df[\"text_raw\"].map(clean_text_minimal)\n",
    "    return df\n",
    "\n",
    "\n",
    "def collect_yandex_reviews_stub(urls: List[str]) -> pd.DataFrame:\n",
    "    rows = [\n",
    "        {\n",
    "            \"doc_id\": \"ya_1\",\n",
    "            \"source\": \"yandex_reviews\",\n",
    "            \"text_raw\": \"Удобно добираться, но внутри тесно. Персонал норм.\",\n",
    "            \"date\": None,\n",
    "            \"url\": urls[0] if urls else None,\n",
    "            \"meta\": {\"rating\": 3, \"place\": \"Заглушка\"},\n",
    "        }\n",
    "    ]\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"text_clean\"] = df[\"text_raw\"].map(clean_text_minimal)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def standardize_docs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    required_cols = [\"doc_id\", \"source\", \"text_raw\", \"date\", \"url\", \"meta\"]\n",
    "    for c in required_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "    df = df[required_cols].copy()\n",
    "    df[\"text_clean\"] = df[\"text_raw\"].map(clean_text_minimal)\n",
    "    return df\n",
    "\n",
    "\n",
    "docs: Optional[pd.DataFrame] = None\n",
    "\n",
    "\n",
    "def build_ui() -> None:\n",
    "    global docs\n",
    "\n",
    "    source_dd = widgets.Dropdown(\n",
    "        options=[(\"VK (стены групп)\", \"vk\"), (\"Сайты\", \"website\"), (\"Яндекс Отзывы\", \"yandex_reviews\")],\n",
    "        value=\"vk\",\n",
    "        description=\"Источник:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"420px\"),\n",
    "    )\n",
    "\n",
    "    input_ta = widgets.Textarea(\n",
    "        value=\"\",\n",
    "        placeholder=\"\"\"Для VK: ссылки на группы или домены через запятую/строку\n",
    "Для сайтов: URL-ы по строкам\n",
    "Для Яндекс Отзывов: URL-ы или ID организаций по строкам\"\"\",\n",
    "        description=\"Ввод вручную:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"820px\", height=\"140px\"),\n",
    "    )\n",
    "\n",
    "    single_link_txt = widgets.Text(\n",
    "        value=\"\",\n",
    "        placeholder=\"Быстрая вставка одной ссылки\",\n",
    "        description=\"Одна ссылка:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"560px\"),\n",
    "    )\n",
    "\n",
    "    links_upload = widgets.FileUpload(\n",
    "        accept=\".xlsx\",\n",
    "        multiple=False,\n",
    "        description=\"XLSX со ссылками\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "    )\n",
    "\n",
    "    since_txt = widgets.Text(\n",
    "        value=\"\",\n",
    "        placeholder=\"YYYY-MM-DD (необязательно)\",\n",
    "        description=\"Период с:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"260px\"),\n",
    "    )\n",
    "\n",
    "    until_txt = widgets.Text(\n",
    "        value=\"\",\n",
    "        placeholder=\"YYYY-MM-DD (необязательно)\",\n",
    "        description=\"Период по:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"260px\"),\n",
    "    )\n",
    "\n",
    "    vk_token_txt = widgets.Password(\n",
    "        value=\"\",\n",
    "        placeholder=\"Токен VK для SOIKA (https://dev.vk.com/api/access-token)\",\n",
    "        description=\"VK токен:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"480px\"),\n",
    "    )\n",
    "\n",
    "    yandex_token_txt = widgets.Password(\n",
    "        value=\"\",\n",
    "        placeholder=\"API-ключ Яндекс Отзывов (developer.tech.yandex.ru)\",\n",
    "        description=\"Yandex токен:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"480px\"),\n",
    "    )\n",
    "\n",
    "    yandex_limit = widgets.IntSlider(\n",
    "        value=300,\n",
    "        min=10,\n",
    "        max=2000,\n",
    "        step=10,\n",
    "        description=\"Лимит отзывов:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"420px\"),\n",
    "    )\n",
    "\n",
    "    selector_main = widgets.Text(\n",
    "        value=\"\",\n",
    "        placeholder=\"Для сайтов: article / .post-content / main\",\n",
    "        description=\"CSS селектор:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"420px\"),\n",
    "    )\n",
    "\n",
    "    min_chars_main = widgets.IntSlider(\n",
    "        value=400,\n",
    "        min=100,\n",
    "        max=4000,\n",
    "        step=50,\n",
    "        description=\"Мин. длина текста:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"420px\"),\n",
    "    )\n",
    "\n",
    "    timeout_main = widgets.IntSlider(\n",
    "        value=20,\n",
    "        min=5,\n",
    "        max=120,\n",
    "        step=5,\n",
    "        description=\"Таймаут (сек):\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width=\"420px\"),\n",
    "    )\n",
    "\n",
    "    demo_btn = widgets.Button(description=\"Загрузить DEMO-корпус\", button_style=\"info\")\n",
    "    run_btn = widgets.Button(description=\"Запустить сбор\", button_style=\"primary\")\n",
    "\n",
    "    out = widgets.Output()\n",
    "\n",
    "    website_opts = widgets.Accordion(children=[widgets.VBox([selector_main, min_chars_main, timeout_main])])\n",
    "    website_opts.set_title(0, \"Опции веб-парсера (для источника 'Сайты')\")\n",
    "\n",
    "    vk_opts = widgets.Accordion(children=[vk_token_txt])\n",
    "    vk_opts.set_title(0, \"Параметры VK / SOIKA\")\n",
    "\n",
    "    yandex_opts = widgets.Accordion(children=[widgets.VBox([yandex_token_txt, yandex_limit])])\n",
    "    yandex_opts.set_title(0, \"Параметры Яндекс Отзывов\")\n",
    "\n",
    "    upload_box = widgets.VBox(\n",
    "        [\n",
    "            widgets.HBox([links_upload, single_link_txt]),\n",
    "            input_ta,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    main_controls = widgets.VBox(\n",
    "        [\n",
    "            source_dd,\n",
    "            upload_box,\n",
    "            widgets.HBox([since_txt, until_txt]),\n",
    "            vk_opts,\n",
    "            yandex_opts,\n",
    "            website_opts,\n",
    "            widgets.HBox([demo_btn, run_btn]),\n",
    "            out,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def get_inputs() -> Dict[str, Any]:\n",
    "        src = source_dd.value\n",
    "        manual_raw = input_ta.value.strip()\n",
    "        single = single_link_txt.value.strip()\n",
    "        since = parse_date_safe(since_txt.value)\n",
    "        until = parse_date_safe(until_txt.value)\n",
    "\n",
    "        manual_items: List[str]\n",
    "        if src == \"vk\":\n",
    "            manual_items = [x.strip() for x in re.split(r\"[\\n,;]+\", manual_raw) if x.strip()]\n",
    "        else:\n",
    "            manual_items = [x.strip() for x in manual_raw.splitlines() if x.strip()]\n",
    "\n",
    "        items = manual_items\n",
    "        if single:\n",
    "            items.append(single)\n",
    "        items.extend(read_links_from_upload(links_upload))\n",
    "\n",
    "        return {\"source\": src, \"items\": items, \"since\": since, \"until\": until}\n",
    "\n",
    "    def load_demo(_=None) -> None:\n",
    "        global docs\n",
    "        with out:\n",
    "            out.clear_output()\n",
    "            demo = pd.concat(\n",
    "                [\n",
    "                    collect_vk_stub([\"demo_group\"], None, None),\n",
    "                    collect_websites_stub(\n",
    "                        [\"https://example.com/article\"],\n",
    "                        selector_main.value or None,\n",
    "                        min_chars_main.value,\n",
    "                        timeout_main.value,\n",
    "                    ),\n",
    "                    collect_yandex_reviews_stub([\"https://example.com/reviews\"]),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "            docs = standardize_docs(demo)\n",
    "            display(Markdown(\"✅ Загружен DEMO-корпус. Ниже — первые строки `docs`.\"))\n",
    "            display(docs.head(10))\n",
    "\n",
    "    def run_pipeline(_=None) -> None:\n",
    "        global docs\n",
    "        cfg = get_inputs()\n",
    "        src = cfg[\"source\"]\n",
    "        items = cfg[\"items\"]\n",
    "\n",
    "        with out:\n",
    "            out.clear_output()\n",
    "            display(\n",
    "                Markdown(\n",
    "                    f\"**Источник:** `{src}`  \\n\"\n",
    "                    + f\"**Элементы ввода:** {len(items)}  \\n\"\n",
    "                    + f\"**Период:** {cfg['since']} — {cfg['until']}\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                if src == \"vk\":\n",
    "                    if VKParser is None:\n",
    "                        raise RuntimeError(_soika_import_error or \"SOIKA недоступна\")\n",
    "                    token = vk_token_txt.value.strip()\n",
    "                    if not token:\n",
    "                        raise ValueError(\"Для VK через SOIKA укажи токен пользователя в отдельном поле.\")\n",
    "                    groups = normalize_vk_domains(items)\n",
    "                    if not groups:\n",
    "                        raise ValueError(\"Список групп VK пуст. Добавь домены или ссылки на группы.\")\n",
    "                    df = collect_vk_soika(groups, token, cfg[\"since\"])\n",
    "                elif src == \"website\":\n",
    "                    if not items:\n",
    "                        raise ValueError(\"Список URL пуст. Добавь ссылки или загрузи XLSX.\")\n",
    "                    df = parse_websites(\n",
    "                        items,\n",
    "                        selector=selector_main.value or None,\n",
    "                        min_chars=int(min_chars_main.value),\n",
    "                        cfg=FetchConfig(timeout=int(timeout_main.value)),\n",
    "                    )\n",
    "                else:\n",
    "                    if not items:\n",
    "                        raise ValueError(\"Список URL пуст. Добавь ссылки или загрузи XLSX.\")\n",
    "                    token = yandex_token_txt.value.strip()\n",
    "                    if not token:\n",
    "                        raise ValueError(\"Для Яндекс Отзывов укажи API-ключ в поле 'Yandex токен'.\")\n",
    "                    org_ids = normalize_yandex_org_ids(items)\n",
    "                    if not org_ids:\n",
    "                        raise ValueError(\"Не удалось извлечь ID организаций. Передай ссылки из Яндекс Карт или числовые ID.\")\n",
    "                    df = collect_yandex_reviews(org_ids, token, limit=int(yandex_limit.value))\n",
    "\n",
    "                docs = standardize_docs(df)\n",
    "                display(Markdown(\"✅ Получена таблица `docs` (в формате для дальнейших разделов).\"))\n",
    "                display(docs)\n",
    "            except Exception as e:\n",
    "                display(Markdown(f\"❌ Ошибка: `{e}`\"))\n",
    "                return\n",
    "\n",
    "    demo_btn.on_click(load_demo)\n",
    "    run_btn.on_click(run_pipeline)\n",
    "\n",
    "    display(main_controls)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "build_ui()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}