{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Краткое описание\n\n",
    "Блокнот собирает тексты из VK, сайтов и Яндекс Отзывов и приводит их к единому формату `docs`. ",
    "Запуск выполняется через UI в следующей ячейке.  \n",
    "Исходный код расположен в скрытой ячейке между описанием и запуском.  \n",
    "Для VK через SOIKA обязательно укажите **токен пользователя** в отдельном поле перед запуском.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n\nimport hashlib\nimport importlib\nimport importlib.util\nimport io\nimport itertools\nimport re\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\nfrom urllib.parse import urlparse\n\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup, Comment\n\nimport ipywidgets as widgets\nfrom IPython.display import display, Markdown\n\n_vk_spec = importlib.util.find_spec(\"soika\")\nif _vk_spec is None:\n    VKParser = None  # type: ignore\n    _soika_import_error = \"SOIKA не установлена\"\nelse:\n    soika = importlib.import_module(\"soika\")\n    VKParser = getattr(soika, \"VKParser\", None)\n    _soika_import_error = None if VKParser is not None else \"В пакете soika нет VKParser\"\n\n_is_colab = importlib.util.find_spec(\"google.colab\") is not None\nif _is_colab:\n    from google.colab import output  # type: ignore\n\n    output.enable_custom_widget_manager()\n\nHTML_TAG_RE = re.compile(r\"<[^>]+>\")\nSPACE_RE = re.compile(r\"\\s+\")\nPUNCT_RUN_RE = re.compile(r\"([!?.,])\\1{2,}\")\n\n\ndef clean_text_minimal(text: str) -> str:\n    if text is None:\n        return \"\"\n    t = str(text)\n    t = HTML_TAG_RE.sub(\" \", t)\n    t = PUNCT_RUN_RE.sub(r\"\\1\\1\", t)\n    t = SPACE_RE.sub(\" \", t).strip()\n    return t\n\n\ndef parse_date_safe(s: str) -> Optional[pd.Timestamp]:\n    s = (s or \"\").strip()\n    if not s:\n        return None\n    try:\n        return pd.to_datetime(s)\n    except Exception:\n        return None\n\n\n# ----------------------------\n# Извлечение содержимого из HTML\n# ----------------------------\n\ndef _strip_noise(soup: BeautifulSoup) -> None:\n    for tag in soup([\"script\", \"style\", \"noscript\", \"svg\", \"canvas\", \"iframe\"]):\n        tag.decompose()\n    for tag in soup.find_all([\"nav\", \"footer\", \"header\", \"aside\"]):\n        tag.decompose()\n    for c in soup.find_all(string=lambda x: isinstance(x, Comment)):\n        c.extract()\n\n\ndef _get_title(soup: BeautifulSoup) -> Optional[str]:\n    for attr, val in [(\"property\", \"og:title\"), (\"name\", \"twitter:title\")]:\n        node = soup.find(\"meta\", attrs={attr: val})\n        if node and node.get(\"content\"):\n            return clean_text_minimal(node[\"content\"])\n\n    if soup.title and soup.title.get_text(strip=True):\n        return clean_text_minimal(soup.title.get_text(\" \", strip=True))\n\n    h1 = soup.find(\"h1\")\n    if h1 and h1.get_text(strip=True):\n        return clean_text_minimal(h1.get_text(\" \", strip=True))\n\n    return None\n\n\ndef _safe_to_datetime(value: Optional[str]) -> Optional[pd.Timestamp]:\n    if not value:\n        return None\n    dt = pd.to_datetime(value, errors=\"coerce\", utc=True)\n    if pd.isna(dt):\n        return None\n    return dt\n\n\ndef _get_date(soup: BeautifulSoup) -> Optional[pd.Timestamp]:\n    meta_candidates = [\n        (\"property\", \"article:published_time\"),\n        (\"property\", \"article:modified_time\"),\n        (\"property\", \"og:updated_time\"),\n        (\"name\", \"pubdate\"),\n        (\"name\", \"publishdate\"),\n        (\"name\", \"timestamp\"),\n        (\"name\", \"date\"),\n        (\"name\", \"DC.date.issued\"),\n        (\"name\", \"DC.Date\"),\n        (\"itemprop\", \"datePublished\"),\n        (\"itemprop\", \"dateModified\"),\n    ]\n    for attr, val in meta_candidates:\n        node = soup.find(\"meta\", attrs={attr: val})\n        if node and node.get(\"content\"):\n            dt = _safe_to_datetime(node[\"content\"])\n            if dt is not None:\n                return dt\n\n    time_tag = soup.find(\"time\")\n    if time_tag:\n        dt = _safe_to_datetime(time_tag.get(\"datetime\"))\n        if dt is not None:\n            return dt\n    return None\n\n\ndef _node_text_len(node) -> int:\n    if not hasattr(node, \"get_text\"):\n        return 0\n    return len(node.get_text(\" \", strip=True))\n\n\ndef _extract_main_text(\n    soup: BeautifulSoup, *, selector: Optional[str] = None, min_chars: int = 400\n) -> Tuple[str, Dict[str, Any]]:\n    meta: Dict[str, Any] = {\"extractor\": None}\n\n    if selector:\n        nodes = soup.select(selector)\n        if nodes:\n            parts = [n.get_text(\"\\n\", strip=True) for n in nodes]\n            text = clean_text_minimal(\"\\n\".join(parts))\n            meta[\"extractor\"] = f\"css:{selector}\"\n            if len(text) >= min_chars:\n                return text, meta\n            meta[\"extractor_fallback\"] = \"too_short\"\n\n    for tag_name in [\"article\", \"main\"]:\n        node = soup.find(tag_name)\n        if node:\n            text = clean_text_minimal(node.get_text(\"\\n\", strip=True))\n            if len(text) >= min_chars:\n                meta[\"extractor\"] = tag_name\n                return text, meta\n\n    candidates = []\n    for key in [\"content\", \"article\", \"post\", \"entry\", \"text\", \"body\", \"main\"]:\n        candidates.extend(soup.find_all(attrs={\"class\": re.compile(key, re.I)}))\n        candidates.extend(soup.find_all(attrs={\"id\": re.compile(key, re.I)}))\n\n    candidates.extend(soup.find_all([\"div\", \"section\"]))\n\n    best = None\n    best_len = 0\n    for node in candidates:\n        l = _node_text_len(node)\n        if l > best_len:\n            best = node\n            best_len = l\n\n    if best is not None and best_len > 0:\n        meta[\"extractor\"] = \"largest_block\"\n        text = clean_text_minimal(best.get_text(\"\\n\", strip=True))\n        return text, meta\n\n    meta[\"extractor\"] = \"none\"\n    return \"\", meta\n\n\n# ----------------------------\n# HTTP helpers\n# ----------------------------\n\n@dataclass\nclass FetchConfig:\n    timeout: int = 20\n    user_agent: str = (\n        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari\"\n    )\n    max_bytes: int = 5_000_000  # 5MB\n\n\ndef _fetch_html(url: str, cfg: FetchConfig, session: requests.Session) -> Tuple[Optional[str], Dict[str, Any]]:\n    meta: Dict[str, Any] = {\"status_code\": None, \"final_url\": None}\n    try:\n        r = session.get(\n            url,\n            headers={\"User-Agent\": cfg.user_agent, \"Accept\": \"text/html,application/xhtml+xml\"},\n            timeout=cfg.timeout,\n            allow_redirects=True,\n        )\n        meta[\"status_code\"] = r.status_code\n        meta[\"final_url\"] = r.url\n        r.raise_for_status()\n\n        content = r.content\n        if content and len(content) > cfg.max_bytes:\n            meta[\"error\"] = f\"response_too_large:{len(content)}\"\n            return None, meta\n\n        if not r.encoding or r.encoding.lower() in {\"iso-8859-1\", \"latin1\", \"ascii\"}:\n            if r.apparent_encoding:\n                r.encoding = r.apparent_encoding\n\n        return r.text, meta\n    except Exception as e:\n        meta[\"error\"] = repr(e)\n        return None, meta\n\n\ndef _stable_id(url: str, text: str) -> str:\n    h = hashlib.sha1((url + \"\\n\" + (text or \"\")[:4000]).encode(\"utf-8\", errors=\"ignore\")).hexdigest()[:16]\n    return f\"web_{h}\"\n\n\ndef parse_websites(\n    urls: List[str],\n    *,\n    selector: Optional[str] = None,\n    min_chars: int = 400,\n    cfg: Optional[FetchConfig] = None,\n) -> pd.DataFrame:\n    cfg = cfg or FetchConfig()\n    rows: List[Dict[str, Any]] = []\n\n    with requests.Session() as session:\n        for url in urls:\n            url = (url or \"\").strip()\n            if not url:\n                continue\n\n            html, fetch_meta = _fetch_html(url, cfg, session=session)\n            if not html:\n                rows.append({\n                    \"doc_id\": _stable_id(url, \"\"),\n                    \"source\": \"website\",\n                    \"text_raw\": \"\",\n                    \"date\": None,\n                    \"url\": url,\n                    \"meta\": {\"fetch\": fetch_meta},\n                })\n                continue\n\n            soup = BeautifulSoup(html, \"lxml\")\n            _strip_noise(soup)\n\n            title = _get_title(soup)\n            date = _get_date(soup)\n            text_raw, extract_meta = _extract_main_text(soup, selector=selector, min_chars=min_chars)\n\n            meta = {\n                \"fetch\": fetch_meta,\n                \"title\": title,\n                \"date_extracted\": date.isoformat() if isinstance(date, pd.Timestamp) else None,\n                \"extraction\": extract_meta,\n                \"domain\": urlparse(fetch_meta.get(\"final_url\") or url).netloc,\n            }\n\n            rows.append({\n                \"doc_id\": _stable_id(fetch_meta.get(\"final_url\") or url, text_raw),\n                \"source\": \"website\",\n                \"text_raw\": text_raw,\n                \"date\": date,\n                \"url\": fetch_meta.get(\"final_url\") or url,\n                \"meta\": meta,\n            })\n\n    df = pd.DataFrame(rows)\n    if df.empty:\n        df = pd.DataFrame(columns=[\"doc_id\", \"source\", \"text_raw\", \"date\", \"url\", \"meta\"])\n\n    df[\"text_clean\"] = df[\"text_raw\"].map(clean_text_minimal)\n    return df\n\n\ndef read_links_from_upload(upload: widgets.FileUpload) -> List[str]:\n    if not upload.value:\n        return []\n    _, file_info = next(iter(upload.value.items()))\n    content = file_info.get(\"content\")\n    if content is None:\n        return []\n    try:\n        df = pd.read_excel(io.BytesIO(content))\n    except Exception:\n        return []\n\n    for col in [\"link\", \"links\", \"url\", \"urls\", \"group\", \"vk\", \"page\"]:\n        if col in df.columns:\n            series = df[col]\n            break\n    else:\n        series = df.iloc[:, 0]\n    return [str(x).strip() for x in series.dropna().astype(str).tolist() if str(x).strip()]\n\n\ndef normalize_vk_domains(items: List[str]) -> List[str]:\n    normalized = []\n    for raw in items:\n        raw = (raw or \"\").strip()\n        if not raw:\n            continue\n        raw = raw.lstrip(\"@\").replace(\"https://\", \"\").replace(\"http://\", \"\")\n        if raw.startswith(\"vk.com/\"):\n            raw = raw.split(\"vk.com/\")[-1]\n        if \"/\" in raw:\n            raw = raw.split(\"/\")[0]\n        normalized.append(raw)\n    return normalized\n\n\ndef collect_vk_soika(groups: List[str], token: str, cutoff: Optional[pd.Timestamp], limit: int = 500) -> pd.DataFrame:\n    if VKParser is None:\n        raise ImportError(_soika_import_error or \"SOIKA не установлена\")\n    cutoff_str = cutoff.strftime(\"%Y-%m-%d\") if cutoff is not None else \"1970-01-01\"\n    frames = []\n    parser = VKParser()\n    message_id_counter = itertools.count(1)\n    for group in groups:\n        df_raw = parser.run_parser(domain=group, access_token=token, cutoff_date=cutoff_str, number_of_messages=limit)\n        if df_raw is None or df_raw.empty:\n            continue\n        df = pd.DataFrame(df_raw)\n        if df.empty:\n            continue\n        df = df.rename(columns={\"text\": \"text_raw\", \"date\": \"date_raw\"})\n        df[\"date\"] = pd.to_datetime(df[\"date_raw\"], errors=\"coerce\")\n\n        df[\"vk_id\"] = df.get(\"id\")\n        df[\"vk_parent_id\"] = df.get(\"parent_message_id\")\n        df[\"message_id\"] = [next(message_id_counter) for _ in range(len(df))]\n        df[\"parent_message_id\"] = None\n\n        id_map = {\n            vk_id: message_id\n            for vk_id, message_id in zip(df[\"vk_id\"], df[\"message_id\"])\n            if pd.notna(vk_id)\n        }\n        df[\"parent_message_id\"] = df[\"vk_parent_id\"].map(id_map)\n\n        df[\"doc_id\"] = df.apply(lambda r: f\"vk_{group}_{r.get('vk_id')}\", axis=1)\n        df[\"url\"] = df.apply(lambda r: f\"https://vk.com/wall{r.get('from_id', '')}_{r.get('vk_id', '')}\", axis=1)\n        df[\"meta\"] = df.apply(lambda r: {\n            \"group\": group,\n            \"type\": r.get(\"type\"),\n            \"likes\": r.get(\"likes.count\"),\n            \"reposts\": r.get(\"reposts.count\"),\n            \"views\": r.get(\"views.count\"),\n            \"link\": r.get(\"link\"),\n            \"vk_id\": r.get(\"vk_id\"),\n            \"vk_parent_id\": r.get(\"vk_parent_id\"),\n            \"message_id\": r.get(\"message_id\"),\n            \"parent_message_id\": r.get(\"parent_message_id\"),\n        }, axis=1)\n        frames.append(\n            df[\n                [\n                    \"doc_id\",\n                    \"text_raw\",\n                    \"date\",\n                    \"url\",\n                    \"meta\",\n                    \"message_id\",\n                    \"parent_message_id\",\n                    \"vk_id\",\n                    \"vk_parent_id\",\n                ]\n            ]\n        )\n    if not frames:\n        return pd.DataFrame(\n            columns=[\"doc_id\", \"text_raw\", \"date\", \"url\", \"meta\", \"message_id\", \"parent_message_id\", \"vk_id\", \"vk_parent_id\"]\n        )\n    out = pd.concat(frames, ignore_index=True)\n    out[\"source\"] = \"vk\"\n    out[\"text_clean\"] = out[\"text_raw\"].map(clean_text_minimal)\n    return out\n\n\ndef collect_vk_stub(group_ids: List[str], since: Optional[pd.Timestamp], until: Optional[pd.Timestamp]) -> pd.DataFrame:\n    rows = [\n        {\n            \"doc_id\": \"vk_1\",\n            \"source\": \"vk\",\n            \"text_raw\": \"Люблю этот район — здесь тихо и много зелени. Но парковки не хватает!!!\",\n            \"date\": pd.Timestamp(\"2025-09-12\"),\n            \"url\": None,\n            \"meta\": {\"group_id\": group_ids[0] if group_ids else None},\n        },\n        {\n            \"doc_id\": \"vk_2\",\n            \"source\": \"vk\",\n            \"text_raw\": \"Опять перекопали улицу у станции. Дойти до остановки — квест.\",\n            \"date\": pd.Timestamp(\"2025-10-03\"),\n            \"url\": None,\n            \"meta\": {\"group_id\": group_ids[0] if group_ids else None},\n        },\n    ]\n    return pd.DataFrame(rows)\n\n\ndef collect_websites_stub(urls: List[str], selector: Optional[str], min_chars: int, timeout: int) -> pd.DataFrame:\n    if urls:\n        return parse_websites(urls, selector=selector, min_chars=min_chars, cfg=FetchConfig(timeout=timeout))\n    rows = [\n        {\n            \"doc_id\": \"web_1\",\n            \"source\": \"website\",\n            \"text_raw\": \"<article>Исторический квартал меняется: появляются новые кафе и мастерские.</article>\",\n            \"date\": None,\n            \"url\": urls[0] if urls else None,\n            \"meta\": {\"title\": \"Заглушка статьи\"},\n        }\n    ]\n    df = pd.DataFrame(rows)\n    df[\"text_clean\"] = df[\"text_raw\"].map(clean_text_minimal)\n    return df\n\n\ndef collect_yandex_reviews_stub(urls: List[str]) -> pd.DataFrame:\n    rows = [\n        {\n            \"doc_id\": \"ya_1\",\n            \"source\": \"yandex_reviews\",\n            \"text_raw\": \"Удобно добираться, но внутри тесно. Персонал норм.\",\n            \"date\": None,\n            \"url\": urls[0] if urls else None,\n            \"meta\": {\"rating\": 3, \"place\": \"Заглушка\"},\n        }\n    ]\n    df = pd.DataFrame(rows)\n    df[\"text_clean\"] = df[\"text_raw\"].map(clean_text_minimal)\n    return df\n\n\ndef standardize_docs(df: pd.DataFrame) -> pd.DataFrame:\n    required_cols = [\"doc_id\", \"source\", \"text_raw\", \"date\", \"url\", \"meta\"]\n    for c in required_cols:\n        if c not in df.columns:\n            df[c] = None\n    df = df[required_cols].copy()\n    df[\"text_clean\"] = df[\"text_raw\"].map(clean_text_minimal)\n    return df\n\n\ndocs: Optional[pd.DataFrame] = None\n\n\ndef build_ui() -> None:\n    global docs\n\n    source_dd = widgets.Dropdown(\n        options=[(\"VK (стены групп)\", \"vk\"), (\"Сайты\", \"website\"), (\"Яндекс Отзывы\", \"yandex_reviews\")],\n        value=\"vk\",\n        description=\"Источник:\",\n        style={\"description_width\": \"initial\"},\n        layout=widgets.Layout(width=\"420px\"),\n    )\n\n    input_ta = widgets.Textarea(\n        value=\"\",\n        placeholder=\"\"\"Для VK: ссылки на группы или домены через запятую/строку\nДля сайтов: URL-ы по строкам\nДля Яндекс Отзывов: URL-ы по строкам\"\"\",\n        description=\"Ввод вручную:\",\n        style={\"description_width\": \"initial\"},\n        layout=widgets.Layout(width=\"820px\", height=\"140px\"),\n    )\n\n    single_link_txt = widgets.Text(\n        value=\"\",\n        placeholder=\"Быстрая вставка одной ссылки\",\n        description=\"Одна ссылка:\",\n        style={\"description_width\": \"initial\"},\n        layout=widgets.Layout(width=\"560px\"),\n    )\n\n    links_upload = widgets.FileUpload(\n        accept=\".xlsx\",\n        multiple=False,\n        description=\"XLSX со ссылками\",\n        style={\"description_width\": \"initial\"},\n    )\n\n    since_txt = widgets.Text(\n        value=\"\",\n        placeholder=\"YYYY-MM-DD (необязательно)\",\n        description=\"Период с:\",\n        style={\"description_width\": \"initial\"},\n        layout=widgets.Layout(width=\"260px\"),\n    )\n\n    until_txt = widgets.Text(\n        value=\"\",\n        placeholder=\"YYYY-MM-DD (необязательно)\",\n        description=\"Период по:\",\n        style={\"description_width\": \"initial\"},\n        layout=widgets.Layout(width=\"260px\"),\n    )\n\n    vk_token_txt = widgets.Password(\n        value=\"\",\n        placeholder=\"Токен VK для SOIKA (https://dev.vk.com/api/access-token)\",\n        description=\"VK токен:\",\n        style={\"description_width\": \"initial\"},\n        layout=widgets.Layout(width=\"480px\"),\n    )\n\n    selector_main = widgets.Text(\n        value=\"\",\n        placeholder=\"Для сайтов: article / .post-content / main\",\n        description=\"CSS селектор:\",\n        style={\"description_width\": \"initial\"},\n        layout=widgets.Layout(width=\"420px\"),\n    )\n\n    min_chars_main = widgets.IntSlider(\n        value=400,\n        min=100,\n        max=4000,\n        step=50,\n        description=\"Мин. длина текста:\",\n        style={\"description_width\": \"initial\"},\n        layout=widgets.Layout(width=\"420px\"),\n    )\n\n    timeout_main = widgets.IntSlider(\n        value=20,\n        min=5,\n        max=120,\n        step=5,\n        description=\"Таймаут (сек):\",\n        style={\"description_width\": \"initial\"},\n        layout=widgets.Layout(width=\"420px\"),\n    )\n\n    demo_btn = widgets.Button(description=\"Загрузить DEMO-корпус\", button_style=\"info\")\n    run_btn = widgets.Button(description=\"Запустить сбор\", button_style=\"primary\")\n\n    out = widgets.Output()\n\n    website_opts = widgets.Accordion(children=[widgets.VBox([selector_main, min_chars_main, timeout_main])])\n    website_opts.set_title(0, \"Опции веб-парсера (для источника 'Сайты')\")\n\n    vk_opts = widgets.Accordion(children=[vk_token_txt])\n    vk_opts.set_title(0, \"Параметры VK / SOIKA\")\n\n    upload_box = widgets.VBox(\n        [\n            widgets.HBox([links_upload, single_link_txt]),\n            input_ta,\n        ]\n    )\n\n    main_controls = widgets.VBox(\n        [\n            source_dd,\n            upload_box,\n            widgets.HBox([since_txt, until_txt]),\n            vk_opts,\n            website_opts,\n            widgets.HBox([demo_btn, run_btn]),\n            out,\n        ]\n    )\n\n    def get_inputs() -> Dict[str, Any]:\n        src = source_dd.value\n        manual_raw = input_ta.value.strip()\n        single = single_link_txt.value.strip()\n        since = parse_date_safe(since_txt.value)\n        until = parse_date_safe(until_txt.value)\n\n        manual_items: List[str]\n        if src == \"vk\":\n            manual_items = [x.strip() for x in re.split(r\"[\\n,;]+\", manual_raw) if x.strip()]\n        else:\n            manual_items = [x.strip() for x in manual_raw.splitlines() if x.strip()]\n\n        items = manual_items\n        if single:\n            items.append(single)\n        items.extend(read_links_from_upload(links_upload))\n\n        return {\"source\": src, \"items\": items, \"since\": since, \"until\": until}\n\n    def load_demo(_=None) -> None:\n        global docs\n        with out:\n            out.clear_output()\n            demo = pd.concat(\n                [\n                    collect_vk_stub([\"demo_group\"], None, None),\n                    collect_websites_stub(\n                        [\"https://example.com/article\"],\n                        selector_main.value or None,\n                        min_chars_main.value,\n                        timeout_main.value,\n                    ),\n                    collect_yandex_reviews_stub([\"https://example.com/reviews\"]),\n                ],\n                ignore_index=True,\n            )\n            docs = standardize_docs(demo)\n            display(Markdown(\"✅ Загружен DEMO-корпус. Ниже — первые строки `docs`.\"))\n            display(docs.head(10))\n\n    def run_pipeline(_=None) -> None:\n        global docs\n        cfg = get_inputs()\n        src = cfg[\"source\"]\n        items = cfg[\"items\"]\n\n        with out:\n            out.clear_output()\n            display(\n                Markdown(\n                    f\"**Источник:** `{src}`  \\n\"\n                    + f\"**Элементы ввода:** {len(items)}  \\n\"\n                    + f\"**Период:** {cfg['since']} — {cfg['until']}\"\n                )\n            )\n\n            try:\n                if src == \"vk\":\n                    if VKParser is None:\n                        raise RuntimeError(_soika_import_error or \"SOIKA недоступна\")\n                    token = vk_token_txt.value.strip()\n                    if not token:\n                        raise ValueError(\"Для VK через SOIKA укажи токен пользователя в отдельном поле.\")\n                    groups = normalize_vk_domains(items)\n                    if not groups:\n                        raise ValueError(\"Список групп VK пуст. Добавь домены или ссылки на группы.\")\n                    df = collect_vk_soika(groups, token, cfg[\"since\"])\n                elif src == \"website\":\n                    if not items:\n                        raise ValueError(\"Список URL пуст. Добавь ссылки или загрузи XLSX.\")\n                    df = parse_websites(\n                        items,\n                        selector=selector_main.value or None,\n                        min_chars=int(min_chars_main.value),\n                        cfg=FetchConfig(timeout=int(timeout_main.value)),\n                    )\n                else:\n                    if not items:\n                        raise ValueError(\"Список URL пуст. Добавь ссылки или загрузи XLSX.\")\n                    df = collect_yandex_reviews_stub(items)\n\n                docs = standardize_docs(df)\n                display(Markdown(\"✅ Получена таблица `docs` (в формате для дальнейших разделов).\"))\n                display(docs)\n            except Exception as e:\n                display(Markdown(f\"❌ Ошибка: `{e}`\"))\n                return\n\n    demo_btn.on_click(load_demo)\n    run_btn.on_click(run_pipeline)\n\n    display(main_controls)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "build_ui()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}